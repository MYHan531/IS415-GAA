---
title: "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study"
title-block-banner: true
author: "Han Ming Yan"
date: 2024-09-02
date-modified: "last-modified"
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    number-sections: true
    number-depth: 3
    code-copy: true
    embed-resources: true
    lightbox: true
    lang: en
abstract: |
  In this study, you are tasked to apply spatial point patterns analysis methods to discover the spatial and spatio-temporal distribution of armed conflict in Myanmar.
keywords: ["Spatial Analysis", "Point Patterns", "First-Order Analysis", "Second-Order Analysis", "Monte Carlo simulation", "Myanmar", "Myanmar Civil War Crisis"]
---

# **Overview**

This exercise focuses on applying geospatial analytics to explore and analyze the impact of the armed conflict in Myanmar. The aim is to use spatial data and analytical techniques to better understand the conflict's dynamics, identify affected regions, and assess the humanitarian implications. This case study offers a real-world application of geospatial tools for social good, demonstrating how data-driven insights can inform decision-making in crisis situations.

## **Data used**

To provide answers to the questions above, the data set that will be used is:

-   ACLED_Myanmar.csv in the data folder

Tools used:

-   [**acledata**](https://acleddata.com/data-export-tool/) **requires Access Key, Given**

::: panel-tabset
## The Task

The specific tasks of this take-home exercise are as follows:

-   Using appropriate function of **sf** and **tidyverse** packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.

-   Using the geospatial data sets prepared, derive quarterly KDE layers.

-   Using the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.

-   Using the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.

-   Using the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.

-   Using appropriate **tmap** functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.

-   Describe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.

## Grading Criteria

-   **Geospatial Data Wrangling (20 marks):** This is an important aspect of geospatial analytics. You will be assessed on your ability to employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the entire geospatial data wrangling processes, including. This is not limited to data import, data extraction, data cleaning and data transformation. Besides assessing your ability to use the R functions, this criterion also includes your ability to clean and derive appropriate variables to meet the analysis need.

<!-- -->

-   **Geospatial Analysis (30 marks):** In this exercise, you are expected to use the appropriate spatial point patterns analysis methods and R packages introduced in class to analysis the geospatial data prepared. You will be assessed on your ability to derive analytical products by using appropriate kernel estimation techniques.

-   **Geovisualisation and geocommunication (20 marks):** In this section, you will be assessed on your ability to communicate Exploratory Spatial Data Analysis and Confirmatory Spatial Data Analysis results in layman friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.

-   **Reproducibility (15 marks):** This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.

-   **Bonus (15 marks):** Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data.
:::

## **Installing and Loading the R packages**

In this hands-on exercise, five R packages will be used, they are:

-   [**sf**](https://r-spatial.github.io/sf/), a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.

-   [**spatstat**](https://spatstat.org/), which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.

-   sparr provides functions to estimate fixed and adaptive kernel

-   [**raster**](https://cran.r-project.org/web/packages/raster/) which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.

-   [**maptools**](https://cran.r-project.org/web/packages/maptools/index.html) which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert *Spatial* objects into *ppp* format of **spatstat**. (Deprecated as of October 2023)

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/index.html) which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using [leaflet](https://leafletjs.com/) API.

Use the code chunk below to install and launch the five R packages.

# Installing maptools

**maptools** is retired as of October 2023 and binary is removed from CRAN. However, we can still download from [**Posit Public Package Manager**](https://packagemanager.posit.co/client/) snapshots by using the code chunk specified

```{r}
#|eval: false # it will no longer run anymore after first run
#|warning: false
install.packages("maptools", repos = "https://packagemanager.posit.co/cran/2023-10-13")
```

```{r}
pacman::p_load(tmap, sf, tidyverse, raster, spatstat, sp, sparr, dplyr)
```

Since it is a .csv file, we will use readr::read_csv() to get the Myanmar data. We will also import the kml data which gives the outline of the country

```{r}
acled_sf <- readr::read_csv("data/myanmar/ACLED_Myanmar.csv") %>%
  st_as_sf(coords = c( "longitude","latitude"),
           crs = 4326) %>%
  st_transform(crs = 32647) %>%
    mutate(event_date = dmy(event_date))
adm0 <- st_read("data/myanmar/geonode-mmr_polbnda_adm0_250k_mimu_1.kml")
adm2 <- st_read("data/myanmar/geonode-mmr_polbnda_adm2_250k_mimu.kml")
adm3 <- st_read("data/myanmar/geonode-mmr_polbnda_adm3_250k_mimu_1.kml")
```

```{r}
head(acled_sf)
```

Next i want to find out about what column names are found in the csv data, so that i can add/remove columns if needed.

```{r}
colnames(acled_sf)
```

since the iso, country and region will definitely be the same for the entire data, we will remove unneccessary data to make reading the data simpler by using the select function from the dplyr package.

```{r}
acled_sf <- acled_sf %>% dplyr::select(-c("iso", "region", "country", "notes"))
```

```{r}
colnames(acled_sf)
```

```{r}
tmap_mode('plot') # interactive view
acled_sf %>% filter(year == 2023 |
                      event_type == 'Political Violence') %>%
  tm_shape() + 
  tm_dots()
# tmap_mode('plot') # static view
```

## Convert acled_sf into their various quarters (10 in total)

```{r}
# Convert event_date to quarters for quarterly analysis, shift quarter to 4th column for easier reference
myanmar_sf <- acled_sf %>%
  mutate(quarter = paste0(year(event_date), " Q", quarter(event_date))) %>%
  dplyr::select(1:3, quarter, everything())
```

### Extract into various data frames based on the quarters

```{r}
# Get all quarters from the data
unique_quarters <- unique(myanmar_sf$quarter)

# Loop through each unique quarter and create separate data frames
for (q in unique_quarters) {
  # Create a filtered data frame for the current quarter
  df <- myanmar_sf %>% filter(quarter == q)
  df_name <- paste0("myanmar_sf_", gsub(" ", "_", q))
  
  # Assign the data frame to a variable with the generated name
  assign(df_name, df)
}

# Check the created data frames
ls(pattern = "myanmar_sf_")
```

we do this so that we have lesser data to work with for each data frame, making it faster overall

## **Converting myanmar_sf frame to sp’s Spatial\* class**

since we have 10 quarters to convert, instead of converting all one by one, i will use a for-each loop to convert them in one go

```{r}
myanmar_sfs <- ls(pattern = "^myanmar_sf_")

# Loop through each of these data frames and convert to Spatial objects
for (df_name in myanmar_sfs) {
  df <- get(df_name)
  sp_object <- as_Spatial(df)
  sp_name <- gsub("sf", "sp", df_name)
  assign(sp_name, sp_object)
}

myanmar_sps <- ls(pattern = "myanmar_sp_")
myanmar_sps
```

## Display one quarter to check

```{r}
myanmar_sp_2021_Q1
```

## **Converting the generic sp format into spatstat’s ppp format**

```{r}
for (df_name in myanmar_sfs) {
  df <- get(df_name)
  # Convert to ppp object
  ppp_object <- as.ppp(df)
  
  # Create a name for the new ppp object
  ppp_name <- gsub("sf", "ppp", df_name)
  
  # Assign the ppp object to a variable with the new name
  assign(ppp_name, ppp_object)
}
myanmar_ppps <- ls(pattern = "myanmar_ppp_")
myanmar_ppps
```

```{r}
myanmar_ppp_2021_Q1
plot(myanmar_ppp_2023_Q3)
```

```{r}
plot(myanmar_ppp)
```

```{r}
summary(myanmar_ppp_2023_Q4)
```

```{r}
any(duplicated(myanmar_ppp_2021_Q1))
```

```{r}
sum(multiplicity(myanmar_ppp_2021_Q1) > 1)
```

### **Creating *owin* objects**

```{r}
for (df_name in myanmar_sfs) {
  ppp_name <- gsub("sf", "ppp", df_name)
  ppp_object <- get(ppp_name)
  owin_object <- as.owin(ppp_object)
  owin_name <- gsub("sf", "owin", df_name)
  assign(owin_name, owin_object)
}
ls(pattern = "myanmar_owin_")
```

```{r}
plot(myanmar_owin_2021_Q1)
```

```{r}
summary(myanmar_owin_2021_Q1)
```

# Second-order Spatial Point Patterns Analysis

## Computing kernel density estimation using automatic bandwidth selection method

```{r}
kde_mynamar_bw <- density(acled_myanmar_ppp, sigma=bw.diggle, edge=TRUE,
                          kernel="gaussian")
plot(kde_mynamar_bw)
```

```{r}
# bw is short form of Bandwidth
bw <- bw.diggle(acled_myanmar_ppp)
bw
```

#### Rescalling KDE values

```{r}
acled_myanmar_ppp.km <- rescale.ppp(acled_myanmar_ppp, 1000, "km")
kde_mynamar_bw <- density(acled_myanmar_ppp, sigma=bw.diggle, edge=TRUE,
                          kernel="gaussian")
plot(kde_mynamar_bw)
```

### **Different Automatic Bandwidth Methods**

```{r}
bw.CvL(acled_myanmar_ppp.km)
```

```{r}
bw.scott(acled_myanmar_ppp.km)
```

```{r}
bw.ppl(acled_myanmar_ppp.km)
```

```{r}
bw.diggle(acled_myanmar_ppp.km)
```

| Function  | Methodology                                | Advantages                                      | Limitations                                          |
|------------------|------------------|------------------|-------------------|
| bw.CvL    | Least Squares Cross-Validation             | Data-driven, adaptive                           | Computationally intensive, sensitive to outliers     |
| bw.scott  | Scott’s Rule of Thumb                      | Simple, quick                                   | Assumes Gaussian distribution, non-adaptive          |
| bw.ppl    | Penalized Profile Likelihood               | Balances fit and smoothness, flexible           | Complex, requires penalty parameter tuning           |
| bw.diggle | Diggle’s Method (Edge-corrected, adaptive) | Tailored for spatial data, handles edge effects | More specific, potentially computationally demanding |

-   **For Quick, General Estimates:** `bw.scott()` is suitable when you need a fast, rule-of-thumb bandwidth without deep customization.

-   **For Data-Driven Precision:** `bw.CvL()` offers a more tailored bandwidth selection by minimizing prediction error but at a higher computational cost.

-   **For Balancing Smoothness and Fit:** `bw.ppl()` is ideal when you want to control the trade-off between overfitting and oversmoothing.

-   **For Spatially Inhomogeneous Data with Edge Concerns:** `bw.diggle()` is preferable when dealing with spatial point patterns that exhibit varying intensities and are subject to boundary effects.

```{r}
acled_myanmar_ppl <- density(acled_myanmar_ppp.km, 
                               sigma=bw.ppl, 
                               edge=TRUE,
                               kernel="gaussian")
par(mfrow=c(1,2))
plot(acled_myanmar.bw, main = "bw.diggle")
plot(acled_myanmar.ppl, main = "bw.ppl")
```

```{r}
par(mfrow=c(2,2))
plot(density(acled_myanmar_ppp.km, 
             sigma=bw.ppl, 
             edge=TRUE, 
             kernel="gaussian"), 
     main="Gaussian")
plot(density(acled_myanmar_ppp.km, 
             sigma=bw.ppl, 
             edge=TRUE, 
             kernel="epanechnikov"), 
     main="Epanechnikov")
plot(density(acled_myanmar_ppp.km, 
             sigma=bw.ppl, 
             edge=TRUE, 
             kernel="quartic"), 
     main="Quartic")
plot(density(acled_myanmar_ppp.km, 
             sigma=bw.ppl, 
             edge=TRUE, 
             kernel="disc"), 
     main="Disc")
```
