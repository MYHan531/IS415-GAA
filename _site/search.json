[
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "",
    "text": "This exercise focuses on applying geospatial analytics to explore and analyze the impact of the armed conflict in Myanmar. The aim is to use spatial data and analytical techniques to better understand the conflict’s dynamics, identify affected regions, and assess the humanitarian implications. This case study offers a real-world application of geospatial tools for social good, demonstrating how data-driven insights can inform decision-making in crisis situations.\n\n\nFor this study, I used the armed conflict data for Myanmar from January 2021 to June 2024, sourced from Armed Conflict Location & Event Data (ACLED). I focused on four primary event types related to conflict:\n\nBattles\nExplosions/Remote Violence\nStrategic Developments\nViolence Against Civilians\n\nTools used:\n\nacledata\n\n\nThe TaskGrading Criteria\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.\n\n\n\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability to employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the entire geospatial data wrangling processes, including. This is not limited to data import, data extraction, data cleaning and data transformation. Besides assessing your ability to use the R functions, this criterion also includes your ability to clean and derive appropriate variables to meet the analysis need.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate spatial point patterns analysis methods and R packages introduced in class to analysis the geospatial data prepared. You will be assessed on your ability to derive analytical products by using appropriate kernel estimation techniques.\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate Exploratory Spatial Data Analysis and Confirmatory Spatial Data Analysis results in layman friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data.\n\n\n\n\n\n\n\nBefore getting into the analysis, I need to load specific R packages that provide essential functions for handling spatial and point pattern data. Packages like sf, spatstat, and sparr offer tools for working with spatial features and conducting point pattern analysis, including kernel density estimation (KDE) and second-order spatial analysis. In total, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nsparr, provides functions to estimate fixed and adaptive kernel\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat. (Deprecated as of October 2023)\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-used",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-used",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "",
    "text": "For this study, I used the armed conflict data for Myanmar from January 2021 to June 2024, sourced from Armed Conflict Location & Event Data (ACLED). I focused on four primary event types related to conflict:\n\nBattles\nExplosions/Remote Violence\nStrategic Developments\nViolence Against Civilians\n\nTools used:\n\nacledata\n\n\nThe TaskGrading Criteria\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.\n\n\n\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability to employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the entire geospatial data wrangling processes, including. This is not limited to data import, data extraction, data cleaning and data transformation. Besides assessing your ability to use the R functions, this criterion also includes your ability to clean and derive appropriate variables to meet the analysis need.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate spatial point patterns analysis methods and R packages introduced in class to analysis the geospatial data prepared. You will be assessed on your ability to derive analytical products by using appropriate kernel estimation techniques.\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate Exploratory Spatial Data Analysis and Confirmatory Spatial Data Analysis results in layman friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#installing-and-loading-the-r-packages",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#installing-and-loading-the-r-packages",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "",
    "text": "Before getting into the analysis, I need to load specific R packages that provide essential functions for handling spatial and point pattern data. Packages like sf, spatstat, and sparr offer tools for working with spatial features and conducting point pattern analysis, including kernel density estimation (KDE) and second-order spatial analysis. In total, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nsparr, provides functions to estimate fixed and adaptive kernel\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat. (Deprecated as of October 2023)\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#importing-and-transforming-the-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#importing-and-transforming-the-data",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.1 Importing and Transforming the Data",
    "text": "2.1 Importing and Transforming the Data\nI then import the armed conflict data (ACLED_Myanmar.csv) and the KML files containing Myanmar’s administrative boundaries. The csv file contains the conflict event data, including latitude, longitude, and the type of event (e.g., battles or explosions). I used the readr::read_csv() function to load the CSV data and convert it into an sf object, which allows me to handle the spatial features (like coordinates) directly in R.\n\nacled_sf &lt;- readr::read_csv(\"data/myanmar/ACLED_Myanmar.csv\") %&gt;%\n  st_as_sf(coords = c( \"longitude\",\"latitude\"),\n           crs = 4326) %&gt;%\n  st_transform(crs = 32647) %&gt;%\n    mutate(event_date = dmy(event_date))\n\nRows: 51553 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): event_id_cnty, event_date, disorder_type, event_type, sub_event_ty...\ndbl (12): year, time_precision, inter1, inter2, interaction, iso, latitude, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNext, I imported the KML files to give me Myanmar’s administrative boundaries at different levels (e.g., provinces, districts). These boundaries will help contextualize the spatial patterns of conflict events within the country’s political borders.\n\nadm0 &lt;- st_read(\"data/myanmar/geonode-mmr_polbnda_adm0_250k_mimu_1.kml\")\n\nReading layer `geonode:mmr_polbnda_adm0_250k_mimu_1' from data source \n  `C:\\myhan\\IS415-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\myanmar\\geonode-mmr_polbnda_adm0_250k_mimu_1.kml' \n  using driver `KML'\nSimple feature collection with 1 feature and 2 fields\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\nadm2 &lt;- st_read(\"data/myanmar/geonode-mmr_polbnda_adm2_250k_mimu.kml\")\n\nReading layer `geonode:mmr_polbnda_adm2_250k_mimu' from data source \n  `C:\\myhan\\IS415-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\myanmar\\geonode-mmr_polbnda_adm2_250k_mimu.kml' \n  using driver `KML'\nSimple feature collection with 80 features and 2 fields\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\nadm3 &lt;- st_read(\"data/myanmar/geonode-mmr_polbnda_adm3_250k_mimu_1.kml\")\n\nReading layer `geonode:mmr_polbnda_adm3_250k_mimu_1' from data source \n  `C:\\myhan\\IS415-GAA\\Take-Home_Ex\\Take-Home_Ex01\\data\\myanmar\\geonode-mmr_polbnda_adm3_250k_mimu_1.kml' \n  using driver `KML'\nSimple feature collection with 330 features and 2 fields\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nhead(acled_sf)\n\nSimple feature collection with 6 features and 30 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 76997.72 ymin: 2428487 xmax: 214961 ymax: 2533434\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 6 × 31\n  event_id_cnty event_date  year time_precision disorder_type         event_type\n  &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;     \n1 MMR64313      2024-06-30  2024              1 Political violence    Battles   \n2 MMR64320      2024-06-30  2024              1 Political violence    Battles   \n3 MMR64321      2024-06-30  2024              1 Political violence    Battles   \n4 MMR64322      2024-06-30  2024              1 Strategic developmen… Strategic…\n5 MMR64323      2024-06-30  2024              1 Political violence    Battles   \n6 MMR64324      2024-06-30  2024              1 Strategic developmen… Strategic…\n# ℹ 25 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, actor2 &lt;chr&gt;, assoc_actor_2 &lt;chr&gt;, inter2 &lt;dbl&gt;,\n#   interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, iso &lt;dbl&gt;, region &lt;chr&gt;,\n#   country &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;, admin3 &lt;chr&gt;, location &lt;chr&gt;,\n#   geo_precision &lt;dbl&gt;, source &lt;chr&gt;, source_scale &lt;chr&gt;, notes &lt;chr&gt;,\n#   fatalities &lt;dbl&gt;, tags &lt;chr&gt;, timestamp &lt;dbl&gt;, population_best &lt;dbl&gt;,\n#   geometry &lt;POINT [m]&gt;\n\n\nNext i want to find out about what column names are found in the csv data, so that i can add/remove columns if needed.\n\ncolnames(acled_sf)\n\n [1] \"event_id_cnty\"      \"event_date\"         \"year\"              \n [4] \"time_precision\"     \"disorder_type\"      \"event_type\"        \n [7] \"sub_event_type\"     \"actor1\"             \"assoc_actor_1\"     \n[10] \"inter1\"             \"actor2\"             \"assoc_actor_2\"     \n[13] \"inter2\"             \"interaction\"        \"civilian_targeting\"\n[16] \"iso\"                \"region\"             \"country\"           \n[19] \"admin1\"             \"admin2\"             \"admin3\"            \n[22] \"location\"           \"geo_precision\"      \"source\"            \n[25] \"source_scale\"       \"notes\"              \"fatalities\"        \n[28] \"tags\"               \"timestamp\"          \"population_best\"   \n[31] \"geometry\""
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-wrangling",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#data-wrangling",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.2 Data Wrangling",
    "text": "2.2 Data Wrangling\nIn this step, I examined the column names of the imported dataset to better understand the variables available and decide which ones were relevant to my analysis. I noticed that variables such as country, region, and ISO codes were redundant since all data points are specific to Myanmar. I removed these columns to simplify the dataset and reduce unnecessary clutter.\n\nacled_sf &lt;- acled_sf %&gt;% dplyr::select(-c(\"iso\", \"region\", \"country\", \"notes\"))\n\nDouble check if the columns are removed\n\ncolnames(acled_sf)\n\n [1] \"event_id_cnty\"      \"event_date\"         \"year\"              \n [4] \"time_precision\"     \"disorder_type\"      \"event_type\"        \n [7] \"sub_event_type\"     \"actor1\"             \"assoc_actor_1\"     \n[10] \"inter1\"             \"actor2\"             \"assoc_actor_2\"     \n[13] \"inter2\"             \"interaction\"        \"civilian_targeting\"\n[16] \"admin1\"             \"admin2\"             \"admin3\"            \n[19] \"location\"           \"geo_precision\"      \"source\"            \n[22] \"source_scale\"       \"fatalities\"         \"tags\"              \n[25] \"timestamp\"          \"population_best\"    \"geometry\"          \n\n\n\n# adm0_polygons &lt;- st_collection_extract(adm0, \"POLYGON\")\nadm2_polygons &lt;- st_collection_extract(adm2, \"POLYGON\")\nadm3_polygons &lt;- st_collection_extract(adm3, \"POLYGON\")\n\n\ntmap_mode('plot') # interactive view\n\ntmap mode set to plotting\n\nacled_sf %&gt;% filter(year == 2023 |\n                      event_type == 'Battles')\n\nSimple feature collection with 21309 features and 26 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -207135 ymin: 1103500 xmax: 604775.1 ymax: 3027042\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 21,309 × 27\n   event_id_cnty event_date  year time_precision disorder_type      event_type\n * &lt;chr&gt;         &lt;date&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     \n 1 MMR64313      2024-06-30  2024              1 Political violence Battles   \n 2 MMR64320      2024-06-30  2024              1 Political violence Battles   \n 3 MMR64321      2024-06-30  2024              1 Political violence Battles   \n 4 MMR64323      2024-06-30  2024              1 Political violence Battles   \n 5 MMR64325      2024-06-30  2024              1 Political violence Battles   \n 6 MMR64326      2024-06-30  2024              1 Political violence Battles   \n 7 MMR64328      2024-06-30  2024              1 Political violence Battles   \n 8 MMR64330      2024-06-30  2024              1 Political violence Battles   \n 9 MMR64331      2024-06-30  2024              1 Political violence Battles   \n10 MMR64332      2024-06-30  2024              1 Political violence Battles   \n# ℹ 21,299 more rows\n# ℹ 21 more variables: sub_event_type &lt;chr&gt;, actor1 &lt;chr&gt;, assoc_actor_1 &lt;chr&gt;,\n#   inter1 &lt;dbl&gt;, actor2 &lt;chr&gt;, assoc_actor_2 &lt;chr&gt;, inter2 &lt;dbl&gt;,\n#   interaction &lt;dbl&gt;, civilian_targeting &lt;chr&gt;, admin1 &lt;chr&gt;, admin2 &lt;chr&gt;,\n#   admin3 &lt;chr&gt;, location &lt;chr&gt;, geo_precision &lt;dbl&gt;, source &lt;chr&gt;,\n#   source_scale &lt;chr&gt;, fatalities &lt;dbl&gt;, tags &lt;chr&gt;, timestamp &lt;dbl&gt;,\n#   population_best &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\ntm_shape(adm2_polygons) + \n  tm_borders(col = \"green\") + \ntm_shape(adm3_polygons) +\n  tm_borders(col = \"black\") +\ntm_shape(adm0) + \n  tm_dots(col = \"blue\", size = 0.1) + \ntm_shape(acled_sf) + \n  tm_dots(col = \"red\", size = 0.1) # Plot the conflict data points in red\n\n\n\n\n\n\n\n\n\n2.2.1 Save data into the rds folder\nWe do this so that we do not need to re-run the previous codes, to keep the derived data in a .rds file\n\n# | eval: false\nwrite_rds(acled_sf, \"data/rds/acled_sf.rds\") # keep in tibble format\n\n\n\n2.2.2 To read back the data, (if we do not have it in our environment yet)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#quarterly-data-segmentation",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#quarterly-data-segmentation",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.3 Quarterly Data Segmentation",
    "text": "2.3 Quarterly Data Segmentation\nSince the objective is to study the temporal patterns of armed conflict over time, I segmented the dataset by quarters (Q1, Q2, Q3, and Q4) for each year. This approach makes it easier to analyze changes in conflict dynamics throughout the years.\n\n# Convert event_date to quarters for quarterly analysis, shift quarter to 4th column for easier reference\nmyanmar_sf &lt;- acled_sf %&gt;%\n  mutate(quarter = paste0(year(event_date), \" Q\", quarter(event_date))) %&gt;%\n  dplyr::select(1:3, quarter, everything())\n\nI further segmented the data based on conflict event types (battles, explosions, strategic developments, and violence against civilians) for each quarter. By doing this, I can isolate different types of conflict events and analyze their distribution over time.\n\n# Get all quarters from the data\nunique_quarters &lt;- unique(myanmar_sf$quarter)\nevent_types &lt;- c(\n  \"Battles\" = \"b\",\n  \"Strategic developments\" = \"sd\",\n  \"Violence against civilians\" = \"vac\",\n  \"Explosions/Remote violence\" = \"erv\"\n)\n\nfor (q in unique_quarters) {\n  \n  df_quarter &lt;- myanmar_sf %&gt;% filter(quarter == q)\n  \n  for (event in names(event_types)) {\n    df_event &lt;- df_quarter %&gt;% filter(event_type == event)\n    \n    # for debugging:\n    #print(paste0(\"Processing quarter: \", q, \", event: \", event, \" - Rows: \", nrow(df_event)))\n    \n    df_name &lt;- paste0(\"myanmar_sf_\", gsub(\" \", \"_\", q), \"_\", event_types[[event]])\n    \n    assign(df_name, df_event)\n  }\n}\n\nls(pattern = \"myanmar_sf_\")\n\n [1] \"myanmar_sf_2021_Q1_b\"   \"myanmar_sf_2021_Q1_erv\" \"myanmar_sf_2021_Q1_sd\" \n [4] \"myanmar_sf_2021_Q1_vac\" \"myanmar_sf_2021_Q2_b\"   \"myanmar_sf_2021_Q2_erv\"\n [7] \"myanmar_sf_2021_Q2_sd\"  \"myanmar_sf_2021_Q2_vac\" \"myanmar_sf_2021_Q3_b\"  \n[10] \"myanmar_sf_2021_Q3_erv\" \"myanmar_sf_2021_Q3_sd\"  \"myanmar_sf_2021_Q3_vac\"\n[13] \"myanmar_sf_2021_Q4_b\"   \"myanmar_sf_2021_Q4_erv\" \"myanmar_sf_2021_Q4_sd\" \n[16] \"myanmar_sf_2021_Q4_vac\" \"myanmar_sf_2022_Q1_b\"   \"myanmar_sf_2022_Q1_erv\"\n[19] \"myanmar_sf_2022_Q1_sd\"  \"myanmar_sf_2022_Q1_vac\" \"myanmar_sf_2022_Q2_b\"  \n[22] \"myanmar_sf_2022_Q2_erv\" \"myanmar_sf_2022_Q2_sd\"  \"myanmar_sf_2022_Q2_vac\"\n[25] \"myanmar_sf_2022_Q3_b\"   \"myanmar_sf_2022_Q3_erv\" \"myanmar_sf_2022_Q3_sd\" \n[28] \"myanmar_sf_2022_Q3_vac\" \"myanmar_sf_2022_Q4_b\"   \"myanmar_sf_2022_Q4_erv\"\n[31] \"myanmar_sf_2022_Q4_sd\"  \"myanmar_sf_2022_Q4_vac\" \"myanmar_sf_2023_Q1_b\"  \n[34] \"myanmar_sf_2023_Q1_erv\" \"myanmar_sf_2023_Q1_sd\"  \"myanmar_sf_2023_Q1_vac\"\n[37] \"myanmar_sf_2023_Q2_b\"   \"myanmar_sf_2023_Q2_erv\" \"myanmar_sf_2023_Q2_sd\" \n[40] \"myanmar_sf_2023_Q2_vac\" \"myanmar_sf_2023_Q3_b\"   \"myanmar_sf_2023_Q3_erv\"\n[43] \"myanmar_sf_2023_Q3_sd\"  \"myanmar_sf_2023_Q3_vac\" \"myanmar_sf_2023_Q4_b\"  \n[46] \"myanmar_sf_2023_Q4_erv\" \"myanmar_sf_2023_Q4_sd\"  \"myanmar_sf_2023_Q4_vac\"\n[49] \"myanmar_sf_2024_Q1_b\"   \"myanmar_sf_2024_Q1_erv\" \"myanmar_sf_2024_Q1_sd\" \n[52] \"myanmar_sf_2024_Q1_vac\" \"myanmar_sf_2024_Q2_b\"   \"myanmar_sf_2024_Q2_erv\"\n[55] \"myanmar_sf_2024_Q2_sd\"  \"myanmar_sf_2024_Q2_vac\"\n\n\nSince I have split the data frames into their respective year, quarter and categories, i will remove the original quarter only data frames and also remove all the empty data frames (0 rows.)\n\nold_quarter_dfs &lt;- ls(pattern = \"^myanmar_sf_\\\\d{4}_Q\\\\d$\")\n\n# Remove the old quarterly data frames\nrm(list = old_quarter_dfs)\nall_dfs &lt;- ls(pattern = \"^myanmar_sf_\")\n\n# Loop through each data frame and remove those with 0 rows\nfor (df_name in all_dfs) {\n  df &lt;- get(df_name)\n  if (nrow(df) == 0) {\n    rm(list = df_name)\n  }\n}\nmyanmar_sfs &lt;- ls(pattern = \"myanmar_sf_\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#converting-myanmar_sf-frame-to-sps-spatial-class",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#converting-myanmar_sf-frame-to-sps-spatial-class",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.4 Converting myanmar_sf frame to sp’s Spatial* class",
    "text": "2.4 Converting myanmar_sf frame to sp’s Spatial* class\nsince we have 14 quarters to convert, instead of converting all one by one, i will use a for-each loop to convert them in one code chunk\n\nmyanmar_sfs &lt;- ls(pattern = \"^myanmar_sf_\")\n\n# Loop through each of these data frames and convert to Spatial objects\nfor (df_name in myanmar_sfs) {\n  df &lt;- get(df_name)\n  sp_object &lt;- as_Spatial(df)\n  sp_name &lt;- gsub(\"sf\", \"sp\", df_name)\n  assign(sp_name, sp_object)\n}\n\nmyanmar_sps &lt;- ls(pattern = \"myanmar_sp_\")\nmyanmar_sps\n\n [1] \"myanmar_sp_2021_Q1_b\"   \"myanmar_sp_2021_Q1_erv\" \"myanmar_sp_2021_Q1_sd\" \n [4] \"myanmar_sp_2021_Q1_vac\" \"myanmar_sp_2021_Q2_b\"   \"myanmar_sp_2021_Q2_erv\"\n [7] \"myanmar_sp_2021_Q2_sd\"  \"myanmar_sp_2021_Q2_vac\" \"myanmar_sp_2021_Q3_b\"  \n[10] \"myanmar_sp_2021_Q3_erv\" \"myanmar_sp_2021_Q3_sd\"  \"myanmar_sp_2021_Q3_vac\"\n[13] \"myanmar_sp_2021_Q4_b\"   \"myanmar_sp_2021_Q4_erv\" \"myanmar_sp_2021_Q4_sd\" \n[16] \"myanmar_sp_2021_Q4_vac\" \"myanmar_sp_2022_Q1_b\"   \"myanmar_sp_2022_Q1_erv\"\n[19] \"myanmar_sp_2022_Q1_sd\"  \"myanmar_sp_2022_Q1_vac\" \"myanmar_sp_2022_Q2_b\"  \n[22] \"myanmar_sp_2022_Q2_erv\" \"myanmar_sp_2022_Q2_sd\"  \"myanmar_sp_2022_Q2_vac\"\n[25] \"myanmar_sp_2022_Q3_b\"   \"myanmar_sp_2022_Q3_erv\" \"myanmar_sp_2022_Q3_sd\" \n[28] \"myanmar_sp_2022_Q3_vac\" \"myanmar_sp_2022_Q4_b\"   \"myanmar_sp_2022_Q4_erv\"\n[31] \"myanmar_sp_2022_Q4_sd\"  \"myanmar_sp_2022_Q4_vac\" \"myanmar_sp_2023_Q1_b\"  \n[34] \"myanmar_sp_2023_Q1_erv\" \"myanmar_sp_2023_Q1_sd\"  \"myanmar_sp_2023_Q1_vac\"\n[37] \"myanmar_sp_2023_Q2_b\"   \"myanmar_sp_2023_Q2_erv\" \"myanmar_sp_2023_Q2_sd\" \n[40] \"myanmar_sp_2023_Q2_vac\" \"myanmar_sp_2023_Q3_b\"   \"myanmar_sp_2023_Q3_erv\"\n[43] \"myanmar_sp_2023_Q3_sd\"  \"myanmar_sp_2023_Q3_vac\" \"myanmar_sp_2023_Q4_b\"  \n[46] \"myanmar_sp_2023_Q4_erv\" \"myanmar_sp_2023_Q4_sd\"  \"myanmar_sp_2023_Q4_vac\"\n[49] \"myanmar_sp_2024_Q1_b\"   \"myanmar_sp_2024_Q1_erv\" \"myanmar_sp_2024_Q1_sd\" \n[52] \"myanmar_sp_2024_Q1_vac\" \"myanmar_sp_2024_Q2_b\"   \"myanmar_sp_2024_Q2_erv\"\n[55] \"myanmar_sp_2024_Q2_sd\"  \"myanmar_sp_2024_Q2_vac\""
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#display-one-quarter-to-check",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#display-one-quarter-to-check",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.5 Display one quarter to check",
    "text": "2.5 Display one quarter to check\nIn this case i used myanmar_sp_2023_Q3_sd\n\nmyanmar_sp_2023_Q3_sd\n\nclass       : SpatialPointsDataFrame \nfeatures    : 724 \nextent      : -197883.4, 518300.4, 1103500, 3027042  (xmin, xmax, ymin, ymax)\ncrs         : +proj=utm +zone=47 +datum=WGS84 +units=m +no_defs \nvariables   : 27\nnames       : event_id_cnty, event_date, year, quarter, time_precision,          disorder_type,             event_type, sub_event_type,                                      actor1,                                                    assoc_actor_1, inter1,                                  actor2,                          assoc_actor_2, inter2, interaction, ... \nmin values  :      MMR49618,      19539, 2023, 2023 Q3,              1, Strategic developments, Strategic developments,        Arrests, ABSDF: All Burma Students' Democratic Front, CNO/CNDF: Chin National Organization/Chin National Defense Force,      1, BFFC: Borderless Freedom Federal Column, Aid Workers (Myanmar); Women (Myanmar),      0,          10, ... \nmax values  :      MMR57190,      19630, 2023, 2023 Q3,              3, Strategic developments, Strategic developments,          Other,                ZRA: Zomi Revolutionary Army,                               Unidentified Anti-Coup Armed Group,      7,                            Yoma GBK PDF,    Women (Myanmar); Teachers (Myanmar),      7,          70, ..."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#converting-the-generic-spatial-format-into-spatstats-ppp-format",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#converting-the-generic-spatial-format-into-spatstats-ppp-format",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.6 Converting the generic spatial format into spatstat’s ppp format",
    "text": "2.6 Converting the generic spatial format into spatstat’s ppp format\nNext, I will convert the spatial data frames into ppp (planar point pattern) objects. This step is crucial because ppp objects are required for performing point pattern analyses like kernel density estimation and second-order analyses in spatstat. A ppp object holds the coordinates of events, along with optional window boundaries and marks (such as event type or time).\n\nfor (df_name in myanmar_sfs) {\n  df &lt;- get(df_name)\n  ppp_object &lt;- as.ppp(df)\n  ppp_name &lt;- gsub(\"sf\", \"ppp\", df_name)\n  assign(ppp_name, ppp_object)\n}\n\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\nWarning in as.ppp.sf(df): only first attribute column is used for marks\n\nmyanmar_ppps &lt;- ls(pattern = \"myanmar_ppp_\")\nmyanmar_ppps\n\n [1] \"myanmar_ppp_2021_Q1_b\"   \"myanmar_ppp_2021_Q1_erv\"\n [3] \"myanmar_ppp_2021_Q1_sd\"  \"myanmar_ppp_2021_Q1_vac\"\n [5] \"myanmar_ppp_2021_Q2_b\"   \"myanmar_ppp_2021_Q2_erv\"\n [7] \"myanmar_ppp_2021_Q2_sd\"  \"myanmar_ppp_2021_Q2_vac\"\n [9] \"myanmar_ppp_2021_Q3_b\"   \"myanmar_ppp_2021_Q3_erv\"\n[11] \"myanmar_ppp_2021_Q3_sd\"  \"myanmar_ppp_2021_Q3_vac\"\n[13] \"myanmar_ppp_2021_Q4_b\"   \"myanmar_ppp_2021_Q4_erv\"\n[15] \"myanmar_ppp_2021_Q4_sd\"  \"myanmar_ppp_2021_Q4_vac\"\n[17] \"myanmar_ppp_2022_Q1_b\"   \"myanmar_ppp_2022_Q1_erv\"\n[19] \"myanmar_ppp_2022_Q1_sd\"  \"myanmar_ppp_2022_Q1_vac\"\n[21] \"myanmar_ppp_2022_Q2_b\"   \"myanmar_ppp_2022_Q2_erv\"\n[23] \"myanmar_ppp_2022_Q2_sd\"  \"myanmar_ppp_2022_Q2_vac\"\n[25] \"myanmar_ppp_2022_Q3_b\"   \"myanmar_ppp_2022_Q3_erv\"\n[27] \"myanmar_ppp_2022_Q3_sd\"  \"myanmar_ppp_2022_Q3_vac\"\n[29] \"myanmar_ppp_2022_Q4_b\"   \"myanmar_ppp_2022_Q4_erv\"\n[31] \"myanmar_ppp_2022_Q4_sd\"  \"myanmar_ppp_2022_Q4_vac\"\n[33] \"myanmar_ppp_2023_Q1_b\"   \"myanmar_ppp_2023_Q1_erv\"\n[35] \"myanmar_ppp_2023_Q1_sd\"  \"myanmar_ppp_2023_Q1_vac\"\n[37] \"myanmar_ppp_2023_Q2_b\"   \"myanmar_ppp_2023_Q2_erv\"\n[39] \"myanmar_ppp_2023_Q2_sd\"  \"myanmar_ppp_2023_Q2_vac\"\n[41] \"myanmar_ppp_2023_Q3_b\"   \"myanmar_ppp_2023_Q3_erv\"\n[43] \"myanmar_ppp_2023_Q3_sd\"  \"myanmar_ppp_2023_Q3_vac\"\n[45] \"myanmar_ppp_2023_Q4_b\"   \"myanmar_ppp_2023_Q4_erv\"\n[47] \"myanmar_ppp_2023_Q4_sd\"  \"myanmar_ppp_2023_Q4_vac\"\n[49] \"myanmar_ppp_2024_Q1_b\"   \"myanmar_ppp_2024_Q1_erv\"\n[51] \"myanmar_ppp_2024_Q1_sd\"  \"myanmar_ppp_2024_Q1_vac\"\n[53] \"myanmar_ppp_2024_Q2_b\"   \"myanmar_ppp_2024_Q2_erv\"\n[55] \"myanmar_ppp_2024_Q2_sd\"  \"myanmar_ppp_2024_Q2_vac\"\n\n\n\nmyanmar_ppp_2023_Q3_b\n\nMarked planar point pattern: 1047 points\nmarks are of storage type  'character'\nwindow: rectangle = [-191409.1, 518300.4] x [1132472.1, 2936770.4] units\n\nplot(myanmar_ppp_2023_Q3_b)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1047 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\n\nsummary(myanmar_ppp_2023_Q3_b)\n\nMarked planar point pattern:  1047 points\nAverage intensity 8.176317e-10 points per square unit\n\nCoordinates are given to 13 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1047 character character \n\nWindow: rectangle = [-191409.1, 518300.4] x [1132472.1, 2936770.4] units\n                    (709700 x 1804000 units)\nWindow area = 1.28053e+12 square units\n\n\n\nany(duplicated(myanmar_ppp_2023_Q3_b))\n\n[1] FALSE\n\n\n\nsum(multiplicity(myanmar_ppp_2023_Q3_b) &gt; 1)\n\n[1] 0"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#creating-owin-objects",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#creating-owin-objects",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "2.7 Creating owin objects",
    "text": "2.7 Creating owin objects\nI also created corresponding owin (window) objects, which define the boundaries of the region under analysis. These owin objects allow me to confine my spatial analysis to the boundaries of Myanmar.\n\nfor (df_name in myanmar_sfs) {\n  ppp_name &lt;- gsub(\"sf\", \"ppp\", df_name)\n  ppp_object &lt;- get(ppp_name)\n  owin_object &lt;- as.owin(ppp_object)\n  owin_name &lt;- gsub(\"sf\", \"owin\", df_name)\n  assign(owin_name, owin_object)\n}\nls(pattern = \"myanmar_owin_\")\n\n [1] \"myanmar_owin_2021_Q1_b\"   \"myanmar_owin_2021_Q1_erv\"\n [3] \"myanmar_owin_2021_Q1_sd\"  \"myanmar_owin_2021_Q1_vac\"\n [5] \"myanmar_owin_2021_Q2_b\"   \"myanmar_owin_2021_Q2_erv\"\n [7] \"myanmar_owin_2021_Q2_sd\"  \"myanmar_owin_2021_Q2_vac\"\n [9] \"myanmar_owin_2021_Q3_b\"   \"myanmar_owin_2021_Q3_erv\"\n[11] \"myanmar_owin_2021_Q3_sd\"  \"myanmar_owin_2021_Q3_vac\"\n[13] \"myanmar_owin_2021_Q4_b\"   \"myanmar_owin_2021_Q4_erv\"\n[15] \"myanmar_owin_2021_Q4_sd\"  \"myanmar_owin_2021_Q4_vac\"\n[17] \"myanmar_owin_2022_Q1_b\"   \"myanmar_owin_2022_Q1_erv\"\n[19] \"myanmar_owin_2022_Q1_sd\"  \"myanmar_owin_2022_Q1_vac\"\n[21] \"myanmar_owin_2022_Q2_b\"   \"myanmar_owin_2022_Q2_erv\"\n[23] \"myanmar_owin_2022_Q2_sd\"  \"myanmar_owin_2022_Q2_vac\"\n[25] \"myanmar_owin_2022_Q3_b\"   \"myanmar_owin_2022_Q3_erv\"\n[27] \"myanmar_owin_2022_Q3_sd\"  \"myanmar_owin_2022_Q3_vac\"\n[29] \"myanmar_owin_2022_Q4_b\"   \"myanmar_owin_2022_Q4_erv\"\n[31] \"myanmar_owin_2022_Q4_sd\"  \"myanmar_owin_2022_Q4_vac\"\n[33] \"myanmar_owin_2023_Q1_b\"   \"myanmar_owin_2023_Q1_erv\"\n[35] \"myanmar_owin_2023_Q1_sd\"  \"myanmar_owin_2023_Q1_vac\"\n[37] \"myanmar_owin_2023_Q2_b\"   \"myanmar_owin_2023_Q2_erv\"\n[39] \"myanmar_owin_2023_Q2_sd\"  \"myanmar_owin_2023_Q2_vac\"\n[41] \"myanmar_owin_2023_Q3_b\"   \"myanmar_owin_2023_Q3_erv\"\n[43] \"myanmar_owin_2023_Q3_sd\"  \"myanmar_owin_2023_Q3_vac\"\n[45] \"myanmar_owin_2023_Q4_b\"   \"myanmar_owin_2023_Q4_erv\"\n[47] \"myanmar_owin_2023_Q4_sd\"  \"myanmar_owin_2023_Q4_vac\"\n[49] \"myanmar_owin_2024_Q1_b\"   \"myanmar_owin_2024_Q1_erv\"\n[51] \"myanmar_owin_2024_Q1_sd\"  \"myanmar_owin_2024_Q1_vac\"\n[53] \"myanmar_owin_2024_Q2_b\"   \"myanmar_owin_2024_Q2_erv\"\n[55] \"myanmar_owin_2024_Q2_sd\"  \"myanmar_owin_2024_Q2_vac\"\n\n\n\nplot(myanmar_owin_2024_Q1_vac)\n\n\n\n\n\n\n\n\n\nsummary(myanmar_owin_2024_Q1_vac)\n\nWindow: rectangle = [-206931.7, 572438.5] x [1359954.3, 3026504.9] units\n                    (779400 x 1667000 units)\nWindow area = 1.29886e+12 square units"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#second-order-spatial-point-patterns-analysis-1",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex01.html#second-order-spatial-point-patterns-analysis-1",
    "title": "Take-Home Exercise 1: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "4.1 Second-Order Spatial Point Patterns Analysis",
    "text": "4.1 Second-Order Spatial Point Patterns Analysis\nFor deeper insights into spatial clustering, I performed Second-Order Spatial Point Patterns Analysis using the K-function and L-function. These functions measure the spatial interaction between points at various distances, helping me detect whether conflict events are clustered, dispersed, or randomly distributed across space.\n\nK-function: This function computes the expected number of events within a given distance from a random event. If the observed K-function lies above the theoretical K-function, it suggests clustering; if below, it indicates dispersion.\nL-function: A linearized version of the K-function, the L-function makes it easier to interpret the results. A plot of the L-function helps visualize whether conflict events tend to cluster or disperse over different scales.\n\nI filtered the conflict data by year to analyze temporal changes in spatial clustering. This helps in understanding how conflict intensity or distribution has evolved over time.\n\nfilter_ppp_by_year &lt;- function(ppp_obj, year) {\n  # Extract the year part from marks or events\n  event_years &lt;- marks(ppp_obj)\n  \n  # Filter the ppp object based on the provided year\n  filtered_ppp &lt;- ppp_obj[event_years == year]\n  \n  return(filtered_ppp)\n}\n\n# Helper function for K-function calculation and plotting\ncompute_k_function &lt;- function(ppp_obj, years) {\n  k_results &lt;- list()\n  \n  for (year in years) {\n    filtered_ppp &lt;- filter_ppp_by_year(ppp_obj, year)\n    \n    if (npoints(filtered_ppp) &gt; 0) {  # Ensure there's data for that year\n      K_result &lt;- Kest(filtered_ppp, correction = \"Ripley\")\n      plot(K_result, . - r ~ r, main = paste(\"K-function for\", year), xlim = c(0, 1000))\n      k_results[[as.character(year)]] &lt;- K_result\n    }\n  }\n  \n  return(k_results)\n}\n# Helper function for L-function calculation and plotting\ncompute_l_function &lt;- function(ppp_obj, years) {\n  l_results &lt;- list()\n  \n  for (year in years) {\n    filtered_ppp &lt;- filter_ppp_by_year(ppp_obj, year)\n    \n    if (npoints(filtered_ppp) &gt; 0) {  # Ensure there's data for that year\n      L_result &lt;- Lest(filtered_ppp, correction = \"Ripley\")\n      plot(L_result, . - r ~ r, main = paste(\"L-function for\", year), xlim = c(0, 1000))\n      l_results[[as.character(year)]] &lt;- L_result\n    }\n  }\n  \n  return(l_results)\n}\n\n\nyears_to_analyze &lt;- c(2021, 2022, 2023, 2024)\n\n# Battles (combined_b_ppp)\nk_battles_results &lt;- compute_k_function(combined_b_ppp, years_to_analyze)\nl_battles_results &lt;- compute_l_function(combined_b_ppp, years_to_analyze)\n\n# Explosions/Remote Violence (combined_erv_ppp)\nk_erv_results &lt;- compute_k_function(combined_erv_ppp, years_to_analyze)\nl_erv_results &lt;- compute_l_function(combined_erv_ppp, years_to_analyze)\n\n# Strategic Developments (combined_sd_ppp)\nk_sd_results &lt;- compute_k_function(combined_sd_ppp, years_to_analyze)\nl_sd_results &lt;- compute_l_function(combined_sd_ppp, years_to_analyze)\n\n# Violence Against Civilians (combined_vac_ppp)\nk_vac_results &lt;- compute_k_function(combined_vac_ppp, years_to_analyze)\nl_vac_results &lt;- compute_l_function(combined_vac_ppp, years_to_analyze)\n\n\n4.1.1 Visualizing the Results\n\n# Plot K-function results for battles by year\nhelper_func_plot_kde_list(k_battles_results, xlab = \"Distance (km)\", ylab = \"K(d)-r\")\n\n# Plot L-function results for strategic developments by year\nhelper_func_plot_kde_list(l_sd_results, xlab = \"Distance (km)\", ylab = \"L(d)-r\")\n\n\n# Create a directory to store the .rds files if it doesn't exist\nif (!dir.exists(\"data/rds\")) {\n  dir.create(\"data/rds\", recursive = TRUE)\n}\nall_objects &lt;- ls()\nfor (obj_name in all_objects) {\n  obj &lt;- get(obj_name)\n  file_path &lt;- paste0(\"data/rds/\", obj_name, \".rds\")\n  saveRDS(obj, file = file_path)\n}\n\n# Verify that the files were saved\nlist.files(\"data/rds\")\n\n  [1] \"acled_sf.rds\"                       \n  [2] \"adm0.rds\"                           \n  [3] \"adm2.rds\"                           \n  [4] \"adm2_polygons.rds\"                  \n  [5] \"adm3.rds\"                           \n  [6] \"adm3_polygons.rds\"                  \n  [7] \"all_dfs.rds\"                        \n  [8] \"all_ppp_objects.rds\"                \n  [9] \"assign_temporal_marks.rds\"          \n [10] \"b_kde_list.rds\"                     \n [11] \"b_ppp_list.rds\"                     \n [12] \"combine_ppp_quarters.rds\"           \n [13] \"combine_ppp_with_owin.rds\"          \n [14] \"combined_b_ppp.rds\"                 \n [15] \"combined_erv_ppp.rds\"               \n [16] \"combined_sd_ppp.rds\"                \n [17] \"combined_vac_ppp.rds\"               \n [18] \"compute_k_function.rds\"             \n [19] \"compute_kde.rds\"                    \n [20] \"compute_l_function.rds\"             \n [21] \"compute_spatio_temporal_kde.rds\"    \n [22] \"convert_to_km.rds\"                  \n [23] \"df.rds\"                             \n [24] \"df_event.rds\"                       \n [25] \"df_name.rds\"                        \n [26] \"df_quarter.rds\"                     \n [27] \"erv_kde_list.rds\"                   \n [28] \"erv_ppp_list.rds\"                   \n [29] \"event.rds\"                          \n [30] \"event_type_mapping.rds\"             \n [31] \"event_types.rds\"                    \n [32] \"F.rds\"                              \n [33] \"F_csr.rds\"                          \n [34] \"filter_ppp_by_year.rds\"             \n [35] \"G.rds\"                              \n [36] \"G_csr.rds\"                          \n [37] \"helper_func_plot_kde_list.rds\"      \n [38] \"K.rds\"                              \n [39] \"k_battles_results.rds\"              \n [40] \"K_csr.rds\"                          \n [41] \"k_erv_results.rds\"                  \n [42] \"k_sd_results.rds\"                   \n [43] \"k_vac_results.rds\"                  \n [44] \"kde_list_battles.rds\"               \n [45] \"kde_list_erv.rds\"                   \n [46] \"kde_list_sd.rds\"                    \n [47] \"kde_list_vac.rds\"                   \n [48] \"kde_result.rds\"                     \n [49] \"kde_result_b.rds\"                   \n [50] \"kde_result_erv.rds\"                 \n [51] \"kde_result_sd.rds\"                  \n [52] \"kde_result_vac.rds\"                 \n [53] \"l_battles_results.rds\"              \n [54] \"l_erv_results.rds\"                  \n [55] \"l_sd_results.rds\"                   \n [56] \"l_vac_results.rds\"                  \n [57] \"myanmar_owin_2021_Q1_b.rds\"         \n [58] \"myanmar_owin_2021_Q1_erv.rds\"       \n [59] \"myanmar_owin_2021_Q1_sd.rds\"        \n [60] \"myanmar_owin_2021_Q1_vac.rds\"       \n [61] \"myanmar_owin_2021_Q2_b.rds\"         \n [62] \"myanmar_owin_2021_Q2_erv.rds\"       \n [63] \"myanmar_owin_2021_Q2_sd.rds\"        \n [64] \"myanmar_owin_2021_Q2_vac.rds\"       \n [65] \"myanmar_owin_2021_Q3_b.rds\"         \n [66] \"myanmar_owin_2021_Q3_erv.rds\"       \n [67] \"myanmar_owin_2021_Q3_sd.rds\"        \n [68] \"myanmar_owin_2021_Q3_vac.rds\"       \n [69] \"myanmar_owin_2021_Q4_b.rds\"         \n [70] \"myanmar_owin_2021_Q4_erv.rds\"       \n [71] \"myanmar_owin_2021_Q4_sd.rds\"        \n [72] \"myanmar_owin_2021_Q4_vac.rds\"       \n [73] \"myanmar_owin_2022_Q1_b.rds\"         \n [74] \"myanmar_owin_2022_Q1_erv.rds\"       \n [75] \"myanmar_owin_2022_Q1_sd.rds\"        \n [76] \"myanmar_owin_2022_Q1_vac.rds\"       \n [77] \"myanmar_owin_2022_Q2_b.rds\"         \n [78] \"myanmar_owin_2022_Q2_erv.rds\"       \n [79] \"myanmar_owin_2022_Q2_sd.rds\"        \n [80] \"myanmar_owin_2022_Q2_vac.rds\"       \n [81] \"myanmar_owin_2022_Q3_b.rds\"         \n [82] \"myanmar_owin_2022_Q3_erv.rds\"       \n [83] \"myanmar_owin_2022_Q3_sd.rds\"        \n [84] \"myanmar_owin_2022_Q3_vac.rds\"       \n [85] \"myanmar_owin_2022_Q4_b.rds\"         \n [86] \"myanmar_owin_2022_Q4_erv.rds\"       \n [87] \"myanmar_owin_2022_Q4_sd.rds\"        \n [88] \"myanmar_owin_2022_Q4_vac.rds\"       \n [89] \"myanmar_owin_2023_Q1_b.rds\"         \n [90] \"myanmar_owin_2023_Q1_erv.rds\"       \n [91] \"myanmar_owin_2023_Q1_sd.rds\"        \n [92] \"myanmar_owin_2023_Q1_vac.rds\"       \n [93] \"myanmar_owin_2023_Q2_b.rds\"         \n [94] \"myanmar_owin_2023_Q2_erv.rds\"       \n [95] \"myanmar_owin_2023_Q2_sd.rds\"        \n [96] \"myanmar_owin_2023_Q2_vac.rds\"       \n [97] \"myanmar_owin_2023_Q3_b.rds\"         \n [98] \"myanmar_owin_2023_Q3_erv.rds\"       \n [99] \"myanmar_owin_2023_Q3_sd.rds\"        \n[100] \"myanmar_owin_2023_Q3_vac.rds\"       \n[101] \"myanmar_owin_2023_Q4_b.rds\"         \n[102] \"myanmar_owin_2023_Q4_erv.rds\"       \n[103] \"myanmar_owin_2023_Q4_sd.rds\"        \n[104] \"myanmar_owin_2023_Q4_vac.rds\"       \n[105] \"myanmar_owin_2024_Q1_b.rds\"         \n[106] \"myanmar_owin_2024_Q1_erv.rds\"       \n[107] \"myanmar_owin_2024_Q1_sd.rds\"        \n[108] \"myanmar_owin_2024_Q1_vac.rds\"       \n[109] \"myanmar_owin_2024_Q2_b.rds\"         \n[110] \"myanmar_owin_2024_Q2_erv.rds\"       \n[111] \"myanmar_owin_2024_Q2_sd.rds\"        \n[112] \"myanmar_owin_2024_Q2_vac.rds\"       \n[113] \"myanmar_ppp_2021_Q1_b.rds\"          \n[114] \"myanmar_ppp_2021_Q1_erv.rds\"        \n[115] \"myanmar_ppp_2021_Q1_sd.rds\"         \n[116] \"myanmar_ppp_2021_Q1_vac.rds\"        \n[117] \"myanmar_ppp_2021_Q2_b.rds\"          \n[118] \"myanmar_ppp_2021_Q2_erv.rds\"        \n[119] \"myanmar_ppp_2021_Q2_sd.rds\"         \n[120] \"myanmar_ppp_2021_Q2_vac.rds\"        \n[121] \"myanmar_ppp_2021_Q3_b.rds\"          \n[122] \"myanmar_ppp_2021_Q3_erv.rds\"        \n[123] \"myanmar_ppp_2021_Q3_sd.rds\"         \n[124] \"myanmar_ppp_2021_Q3_vac.rds\"        \n[125] \"myanmar_ppp_2021_Q4_b.rds\"          \n[126] \"myanmar_ppp_2021_Q4_erv.rds\"        \n[127] \"myanmar_ppp_2021_Q4_sd.rds\"         \n[128] \"myanmar_ppp_2021_Q4_vac.rds\"        \n[129] \"myanmar_ppp_2022_Q1_b.rds\"          \n[130] \"myanmar_ppp_2022_Q1_erv.rds\"        \n[131] \"myanmar_ppp_2022_Q1_sd.rds\"         \n[132] \"myanmar_ppp_2022_Q1_vac.rds\"        \n[133] \"myanmar_ppp_2022_Q2_b.rds\"          \n[134] \"myanmar_ppp_2022_Q2_erv.rds\"        \n[135] \"myanmar_ppp_2022_Q2_sd.rds\"         \n[136] \"myanmar_ppp_2022_Q2_vac.rds\"        \n[137] \"myanmar_ppp_2022_Q3_b.rds\"          \n[138] \"myanmar_ppp_2022_Q3_erv.rds\"        \n[139] \"myanmar_ppp_2022_Q3_sd.rds\"         \n[140] \"myanmar_ppp_2022_Q3_vac.rds\"        \n[141] \"myanmar_ppp_2022_Q4_b.rds\"          \n[142] \"myanmar_ppp_2022_Q4_erv.rds\"        \n[143] \"myanmar_ppp_2022_Q4_sd.rds\"         \n[144] \"myanmar_ppp_2022_Q4_vac.rds\"        \n[145] \"myanmar_ppp_2023_Q1_b.rds\"          \n[146] \"myanmar_ppp_2023_Q1_erv.rds\"        \n[147] \"myanmar_ppp_2023_Q1_sd.rds\"         \n[148] \"myanmar_ppp_2023_Q1_vac.rds\"        \n[149] \"myanmar_ppp_2023_Q2_b.rds\"          \n[150] \"myanmar_ppp_2023_Q2_erv.rds\"        \n[151] \"myanmar_ppp_2023_Q2_sd.rds\"         \n[152] \"myanmar_ppp_2023_Q2_vac.rds\"        \n[153] \"myanmar_ppp_2023_Q3_b.rds\"          \n[154] \"myanmar_ppp_2023_Q3_erv.rds\"        \n[155] \"myanmar_ppp_2023_Q3_sd.rds\"         \n[156] \"myanmar_ppp_2023_Q3_vac.rds\"        \n[157] \"myanmar_ppp_2023_Q4_b.rds\"          \n[158] \"myanmar_ppp_2023_Q4_erv.rds\"        \n[159] \"myanmar_ppp_2023_Q4_sd.rds\"         \n[160] \"myanmar_ppp_2023_Q4_vac.rds\"        \n[161] \"myanmar_ppp_2024_Q1_b.rds\"          \n[162] \"myanmar_ppp_2024_Q1_erv.rds\"        \n[163] \"myanmar_ppp_2024_Q1_sd.rds\"         \n[164] \"myanmar_ppp_2024_Q1_vac.rds\"        \n[165] \"myanmar_ppp_2024_Q2_b.rds\"          \n[166] \"myanmar_ppp_2024_Q2_erv.rds\"        \n[167] \"myanmar_ppp_2024_Q2_sd.rds\"         \n[168] \"myanmar_ppp_2024_Q2_vac.rds\"        \n[169] \"myanmar_ppp_ppl.rds\"                \n[170] \"myanmar_ppp_temp_km_2021_Q1_b.rds\"  \n[171] \"myanmar_ppp_temp_km_2021_Q1_erv.rds\"\n[172] \"myanmar_ppp_temp_km_2021_Q1_sd.rds\" \n[173] \"myanmar_ppp_temp_km_2021_Q1_vac.rds\"\n[174] \"myanmar_ppp_temp_km_2021_Q2_b.rds\"  \n[175] \"myanmar_ppp_temp_km_2021_Q2_erv.rds\"\n[176] \"myanmar_ppp_temp_km_2021_Q2_sd.rds\" \n[177] \"myanmar_ppp_temp_km_2021_Q2_vac.rds\"\n[178] \"myanmar_ppp_temp_km_2021_Q3_b.rds\"  \n[179] \"myanmar_ppp_temp_km_2021_Q3_erv.rds\"\n[180] \"myanmar_ppp_temp_km_2021_Q3_sd.rds\" \n[181] \"myanmar_ppp_temp_km_2021_Q3_vac.rds\"\n[182] \"myanmar_ppp_temp_km_2021_Q4_b.rds\"  \n[183] \"myanmar_ppp_temp_km_2021_Q4_erv.rds\"\n[184] \"myanmar_ppp_temp_km_2021_Q4_sd.rds\" \n[185] \"myanmar_ppp_temp_km_2021_Q4_vac.rds\"\n[186] \"myanmar_ppp_temp_km_2022_Q1_b.rds\"  \n[187] \"myanmar_ppp_temp_km_2022_Q1_erv.rds\"\n[188] \"myanmar_ppp_temp_km_2022_Q1_sd.rds\" \n[189] \"myanmar_ppp_temp_km_2022_Q1_vac.rds\"\n[190] \"myanmar_ppp_temp_km_2022_Q2_b.rds\"  \n[191] \"myanmar_ppp_temp_km_2022_Q2_erv.rds\"\n[192] \"myanmar_ppp_temp_km_2022_Q2_sd.rds\" \n[193] \"myanmar_ppp_temp_km_2022_Q2_vac.rds\"\n[194] \"myanmar_ppp_temp_km_2022_Q3_b.rds\"  \n[195] \"myanmar_ppp_temp_km_2022_Q3_erv.rds\"\n[196] \"myanmar_ppp_temp_km_2022_Q3_sd.rds\" \n[197] \"myanmar_ppp_temp_km_2022_Q3_vac.rds\"\n[198] \"myanmar_ppp_temp_km_2022_Q4_b.rds\"  \n[199] \"myanmar_ppp_temp_km_2022_Q4_erv.rds\"\n[200] \"myanmar_ppp_temp_km_2022_Q4_sd.rds\" \n[201] \"myanmar_ppp_temp_km_2022_Q4_vac.rds\"\n[202] \"myanmar_ppp_temp_km_2023_Q1_b.rds\"  \n[203] \"myanmar_ppp_temp_km_2023_Q1_erv.rds\"\n[204] \"myanmar_ppp_temp_km_2023_Q1_sd.rds\" \n[205] \"myanmar_ppp_temp_km_2023_Q1_vac.rds\"\n[206] \"myanmar_ppp_temp_km_2023_Q2_b.rds\"  \n[207] \"myanmar_ppp_temp_km_2023_Q2_erv.rds\"\n[208] \"myanmar_ppp_temp_km_2023_Q2_sd.rds\" \n[209] \"myanmar_ppp_temp_km_2023_Q2_vac.rds\"\n[210] \"myanmar_ppp_temp_km_2023_Q3_b.rds\"  \n[211] \"myanmar_ppp_temp_km_2023_Q3_erv.rds\"\n[212] \"myanmar_ppp_temp_km_2023_Q3_sd.rds\" \n[213] \"myanmar_ppp_temp_km_2023_Q3_vac.rds\"\n[214] \"myanmar_ppp_temp_km_2023_Q4_b.rds\"  \n[215] \"myanmar_ppp_temp_km_2023_Q4_erv.rds\"\n[216] \"myanmar_ppp_temp_km_2023_Q4_sd.rds\" \n[217] \"myanmar_ppp_temp_km_2023_Q4_vac.rds\"\n[218] \"myanmar_ppp_temp_km_2024_Q1_b.rds\"  \n[219] \"myanmar_ppp_temp_km_2024_Q1_erv.rds\"\n[220] \"myanmar_ppp_temp_km_2024_Q1_sd.rds\" \n[221] \"myanmar_ppp_temp_km_2024_Q1_vac.rds\"\n[222] \"myanmar_ppp_temp_km_2024_Q2_b.rds\"  \n[223] \"myanmar_ppp_temp_km_2024_Q2_erv.rds\"\n[224] \"myanmar_ppp_temp_km_2024_Q2_sd.rds\" \n[225] \"myanmar_ppp_temp_km_2024_Q2_vac.rds\"\n[226] \"myanmar_ppps.rds\"                   \n[227] \"myanmar_sf.rds\"                     \n[228] \"myanmar_sf_2021_Q1_b.rds\"           \n[229] \"myanmar_sf_2021_Q1_erv.rds\"         \n[230] \"myanmar_sf_2021_Q1_sd.rds\"          \n[231] \"myanmar_sf_2021_Q1_vac.rds\"         \n[232] \"myanmar_sf_2021_Q2_b.rds\"           \n[233] \"myanmar_sf_2021_Q2_erv.rds\"         \n[234] \"myanmar_sf_2021_Q2_sd.rds\"          \n[235] \"myanmar_sf_2021_Q2_vac.rds\"         \n[236] \"myanmar_sf_2021_Q3_b.rds\"           \n[237] \"myanmar_sf_2021_Q3_erv.rds\"         \n[238] \"myanmar_sf_2021_Q3_sd.rds\"          \n[239] \"myanmar_sf_2021_Q3_vac.rds\"         \n[240] \"myanmar_sf_2021_Q4_b.rds\"           \n[241] \"myanmar_sf_2021_Q4_erv.rds\"         \n[242] \"myanmar_sf_2021_Q4_sd.rds\"          \n[243] \"myanmar_sf_2021_Q4_vac.rds\"         \n[244] \"myanmar_sf_2022_Q1_b.rds\"           \n[245] \"myanmar_sf_2022_Q1_erv.rds\"         \n[246] \"myanmar_sf_2022_Q1_sd.rds\"          \n[247] \"myanmar_sf_2022_Q1_vac.rds\"         \n[248] \"myanmar_sf_2022_Q2_b.rds\"           \n[249] \"myanmar_sf_2022_Q2_erv.rds\"         \n[250] \"myanmar_sf_2022_Q2_sd.rds\"          \n[251] \"myanmar_sf_2022_Q2_vac.rds\"         \n[252] \"myanmar_sf_2022_Q3_b.rds\"           \n[253] \"myanmar_sf_2022_Q3_erv.rds\"         \n[254] \"myanmar_sf_2022_Q3_sd.rds\"          \n[255] \"myanmar_sf_2022_Q3_vac.rds\"         \n[256] \"myanmar_sf_2022_Q4_b.rds\"           \n[257] \"myanmar_sf_2022_Q4_erv.rds\"         \n[258] \"myanmar_sf_2022_Q4_sd.rds\"          \n[259] \"myanmar_sf_2022_Q4_vac.rds\"         \n[260] \"myanmar_sf_2023_Q1_b.rds\"           \n[261] \"myanmar_sf_2023_Q1_erv.rds\"         \n[262] \"myanmar_sf_2023_Q1_sd.rds\"          \n[263] \"myanmar_sf_2023_Q1_vac.rds\"         \n[264] \"myanmar_sf_2023_Q2_b.rds\"           \n[265] \"myanmar_sf_2023_Q2_erv.rds\"         \n[266] \"myanmar_sf_2023_Q2_sd.rds\"          \n[267] \"myanmar_sf_2023_Q2_vac.rds\"         \n[268] \"myanmar_sf_2023_Q3_b.rds\"           \n[269] \"myanmar_sf_2023_Q3_erv.rds\"         \n[270] \"myanmar_sf_2023_Q3_sd.rds\"          \n[271] \"myanmar_sf_2023_Q3_vac.rds\"         \n[272] \"myanmar_sf_2023_Q4_b.rds\"           \n[273] \"myanmar_sf_2023_Q4_erv.rds\"         \n[274] \"myanmar_sf_2023_Q4_sd.rds\"          \n[275] \"myanmar_sf_2023_Q4_vac.rds\"         \n[276] \"myanmar_sf_2024_Q1_b.rds\"           \n[277] \"myanmar_sf_2024_Q1_erv.rds\"         \n[278] \"myanmar_sf_2024_Q1_sd.rds\"          \n[279] \"myanmar_sf_2024_Q1_vac.rds\"         \n[280] \"myanmar_sf_2024_Q2_b.rds\"           \n[281] \"myanmar_sf_2024_Q2_erv.rds\"         \n[282] \"myanmar_sf_2024_Q2_sd.rds\"          \n[283] \"myanmar_sf_2024_Q2_vac.rds\"         \n[284] \"myanmar_sfs.rds\"                    \n[285] \"myanmar_sp_2021_Q1_b.rds\"           \n[286] \"myanmar_sp_2021_Q1_erv.rds\"         \n[287] \"myanmar_sp_2021_Q1_sd.rds\"          \n[288] \"myanmar_sp_2021_Q1_vac.rds\"         \n[289] \"myanmar_sp_2021_Q2_b.rds\"           \n[290] \"myanmar_sp_2021_Q2_erv.rds\"         \n[291] \"myanmar_sp_2021_Q2_sd.rds\"          \n[292] \"myanmar_sp_2021_Q2_vac.rds\"         \n[293] \"myanmar_sp_2021_Q3_b.rds\"           \n[294] \"myanmar_sp_2021_Q3_erv.rds\"         \n[295] \"myanmar_sp_2021_Q3_sd.rds\"          \n[296] \"myanmar_sp_2021_Q3_vac.rds\"         \n[297] \"myanmar_sp_2021_Q4_b.rds\"           \n[298] \"myanmar_sp_2021_Q4_erv.rds\"         \n[299] \"myanmar_sp_2021_Q4_sd.rds\"          \n[300] \"myanmar_sp_2021_Q4_vac.rds\"         \n[301] \"myanmar_sp_2022_Q1_b.rds\"           \n[302] \"myanmar_sp_2022_Q1_erv.rds\"         \n[303] \"myanmar_sp_2022_Q1_sd.rds\"          \n[304] \"myanmar_sp_2022_Q1_vac.rds\"         \n[305] \"myanmar_sp_2022_Q2_b.rds\"           \n[306] \"myanmar_sp_2022_Q2_erv.rds\"         \n[307] \"myanmar_sp_2022_Q2_sd.rds\"          \n[308] \"myanmar_sp_2022_Q2_vac.rds\"         \n[309] \"myanmar_sp_2022_Q3_b.rds\"           \n[310] \"myanmar_sp_2022_Q3_erv.rds\"         \n[311] \"myanmar_sp_2022_Q3_sd.rds\"          \n[312] \"myanmar_sp_2022_Q3_vac.rds\"         \n[313] \"myanmar_sp_2022_Q4_b.rds\"           \n[314] \"myanmar_sp_2022_Q4_erv.rds\"         \n[315] \"myanmar_sp_2022_Q4_sd.rds\"          \n[316] \"myanmar_sp_2022_Q4_vac.rds\"         \n[317] \"myanmar_sp_2023_Q1_b.rds\"           \n[318] \"myanmar_sp_2023_Q1_erv.rds\"         \n[319] \"myanmar_sp_2023_Q1_sd.rds\"          \n[320] \"myanmar_sp_2023_Q1_vac.rds\"         \n[321] \"myanmar_sp_2023_Q2_b.rds\"           \n[322] \"myanmar_sp_2023_Q2_erv.rds\"         \n[323] \"myanmar_sp_2023_Q2_sd.rds\"          \n[324] \"myanmar_sp_2023_Q2_vac.rds\"         \n[325] \"myanmar_sp_2023_Q3_b.rds\"           \n[326] \"myanmar_sp_2023_Q3_erv.rds\"         \n[327] \"myanmar_sp_2023_Q3_sd.rds\"          \n[328] \"myanmar_sp_2023_Q3_vac.rds\"         \n[329] \"myanmar_sp_2023_Q4_b.rds\"           \n[330] \"myanmar_sp_2023_Q4_erv.rds\"         \n[331] \"myanmar_sp_2023_Q4_sd.rds\"          \n[332] \"myanmar_sp_2023_Q4_vac.rds\"         \n[333] \"myanmar_sp_2024_Q1_b.rds\"           \n[334] \"myanmar_sp_2024_Q1_erv.rds\"         \n[335] \"myanmar_sp_2024_Q1_sd.rds\"          \n[336] \"myanmar_sp_2024_Q1_vac.rds\"         \n[337] \"myanmar_sp_2024_Q2_b.rds\"           \n[338] \"myanmar_sp_2024_Q2_erv.rds\"         \n[339] \"myanmar_sp_2024_Q2_sd.rds\"          \n[340] \"myanmar_sp_2024_Q2_vac.rds\"         \n[341] \"myanmar_sps.rds\"                    \n[342] \"non_km_objects.rds\"                 \n[343] \"old_quarter_dfs.rds\"                \n[344] \"owin_name.rds\"                      \n[345] \"owin_object.rds\"                    \n[346] \"plot_combined_ppp.rds\"              \n[347] \"ppp_km.rds\"                         \n[348] \"ppp_name.rds\"                       \n[349] \"ppp_object.rds\"                     \n[350] \"ppp_object_names.rds\"               \n[351] \"ppp_temps.rds\"                      \n[352] \"q.rds\"                              \n[353] \"quarter_labels.rds\"                 \n[354] \"quarter_time_mapping.rds\"           \n[355] \"sd_kde_list.rds\"                    \n[356] \"sd_ppp_list.rds\"                    \n[357] \"sigma_value.rds\"                    \n[358] \"sp_name.rds\"                        \n[359] \"sp_object.rds\"                      \n[360] \"unique_quarters.rds\"                \n[361] \"vac_kde_list.rds\"                   \n[362] \"vac_ppp_list.rds\"                   \n[363] \"years_to_analyze.rds\""
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "",
    "text": "pacman::p_load(tmap, sf, tidyverse, spdep, knitr, GWmodel, dplyr)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#import-hunan_shapefile",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#import-hunan_shapefile",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "2.1 Import hunan_shapefile",
    "text": "2.1 Import hunan_shapefile\n\n# eval: false\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\IS415-GAA\\In-Class_Ex\\In-Class_Ex05\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#import-hunan2012.csv",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#import-hunan2012.csv",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "2.2 Import hunan2012.csv",
    "text": "2.2 Import hunan2012.csv\n\n# eval: false\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#perform-relational-joins-for-the-2-hunan-data-frames",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#perform-relational-joins-for-the-2-hunan-data-frames",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "2.3 Perform Relational Joins for the 2 Hunan data frames",
    "text": "2.3 Perform Relational Joins for the 2 Hunan data frames\n\n# eval: false # since we will have the rds file, we will not need to run this code again\n#| \n# all of hunan_sf plus any similar colnames from hunan2012\nhunan_sf &lt;- left_join(hunan_sf,hunan2012) %&gt;% \n  dplyr::select(1:3, 7, 15, 16, 31, 32) # only see these columns from the hunan_sf\n\nJoining with `by = join_by(County)`\n\n\n\n2.3.1 Save data into the rds folder\nWe do this so that we do not need to re-run the previous codes, to keep the derived data in a .rds file\n\n# eval: false\nwrite_rds(hunan_sf, \"data/rds/hunan_sf.rds\") # keep in tibble format\n\n\n\n2.3.2 To read back the data, (if we do not have it in our environment yet)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#determine-adaptive-bandwidth",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#determine-adaptive-bandwidth",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "4.1 Determine adaptive bandwidth",
    "text": "4.1 Determine adaptive bandwidth\n\nCross-ValidationAkaike information criterion (AIC)\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"CV\",\n                 adaptive = TRUE, # false means you are calculating fixed distance\n                 kernel = \"bisquare\",\n                 longlat = T)\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n# the recommeneded number of Geo measures is 22 as seen in the output\n\n\n\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"AIC\",\n                 adaptive = TRUE, # false means you are calculating fixed distance\n                 kernel = \"bisquare\",\n                 longlat = T) # impt: in the package, it will transform your data into great circle, in kilometres, NOT metre\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#computing-geographically-weighted-summary-statistics",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#computing-geographically-weighted-summary-statistics",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "4.2 Computing geographically weighted summary statistics",
    "text": "4.2 Computing geographically weighted summary statistics\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",                \n               bw = bw_AIC,                \n               kernel = \"bisquare\",                \n               adaptive = TRUE,                \n               longlat = T)\n# fast as we only have 88 rows of data\n# the one that we need in gwstat is the SDF (SpatialPolygonsDataFrame)\n\n\ngwstat[[\"SDF\"]]\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 88 \nextent      : 108.7831, 114.2544, 24.6342, 30.12812  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs \nvariables   : 5\nnames       :         GDPPC_LM,        GDPPC_LSD,       GDPPC_LVar,         GDPPC_LSKe,         GDPPC_LCV \nmin values  : 13688.6986033259, 4282.59917616925, 18340655.7037255, -0.215059890053628, 0.200050258645349 \nmax values  : 49005.8382943034, 22568.8411539952, 509352591.034267,    3.7525953469342, 0.801815253056721 \n\n# view(gwstat[[\"SDF\"]]@data) # this one includes itself unlike the previous sp data"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#preparing-the-output-data",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#preparing-the-output-data",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "4.3 Preparing the output data",
    "text": "4.3 Preparing the output data\nThe Code chunk below is used to extract SDf data table from gwss object output from gwss(). It will be converted to a data frame by using base::as.data.frame\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n# Next we will use cbind() to append the newly derived data.frame onto hunan_sf sf data.frame\nhunan_gstat &lt;-  cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#visualising-geographically-weighted-summary-statistics",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "4.4 Visualising geographically weighted summary statistics",
    "text": "4.4 Visualising geographically weighted summary statistics\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\", n=5, style = \"quantile\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weighted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50,\n            legend.width = 1.50,\n            frame = TRUE)\n\n\n\n\n\n\n\n#interactive view, avoid as much as possible, it might hang the browser as the memory is too high when there is too many interactive views, even 2 of them can crash the browser and RStudios"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#determine-fixed-bandwidth",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex05.html#determine-fixed-bandwidth",
    "title": "In-Class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "5.1 Determine fixed bandwidth",
    "text": "5.1 Determine fixed bandwidth\nto do so, it is the same as the adaptive bandwidth except that the adaptive condition will now be FALSE.\n\nCross-ValidationAkaike information criterion (AIC)\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"CV\",\n                 adaptive = FALSE, # false means you are calculating fixed distance, TRUE means you are calculating adaptive distances\n                 kernel = \"bisquare\",\n                 longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\n# 76.29123 CV score: 13442844676, it gave 2 different distances to the AIC, quite a big difference compared to the adaptive bandwidth\n\n# there is no right or wrong answer, up to you to choose. but must visualise the result.\n\n\n\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1,\n                 data = hunan_sp,\n                 approach = \"AIC\",\n                 adaptive = FALSE,\n                 kernel = \"bisquare\",\n                 longlat = T) # impt: in the package, it will transform your data into great circle, in kilometres, NOT metre\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\n# 160.5517 AICc value: 1919.391"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex02/In-Class_Ex02.html",
    "href": "In-Class_Ex/In-Class_Ex02/In-Class_Ex02.html",
    "title": "In-Class Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "The TaskTo import shapefileTo import kml\n\n\n\nVisit data.gov.sg and download Master Plan 2019 Subzone Boundary (No Sea) from the portal. You are required to download both kml files.\nMove MPSZ-2019 shapefile provided for In-Class Exercise 1 folder on elearn to data sub-folder of In-Class_Ex02.\nWrite a code chunk to import Master Plan 2019 Subzone Boundary (No Sea) kml and MPSZ-2019 into sf simple feature data.frame.\n\n\n\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\npacman::p_load(sf, tidyverse)\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz14_shp.\n\nmpsz19_shp &lt;- st_read(dsn = \"data\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\myhan\\IS415-GAA\\In-Class_Ex\\In-Class_Ex02\\data' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\npacman::p_load(sf, tidyverse)\n\n\nThe code chunk imports kml file. ::: {style=“font-size: 1.50em”}\n\n\nDeleting source `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting layer `MasterPlan2019SubzoneBoundaryNoSeaKML' to data source \n  `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting 332 features with 6 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\n\nPre-School Shape\n\n\n\npreschool &lt;- st_write(mpsz19_shp, \n                       \"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\",\n                       delete_dsn = TRUE) #incomplete\n\nDeleting source `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting layer `MasterPlan2019SubzoneBoundaryNoSeaKML' to data source \n  `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting 332 features with 6 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\n\n\nThe Code\n\n\n\npopdata &lt;- readr::read_csv(\"data/aspatial/respopagesextod2023.csv\")\n\nRows: 100928 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nData Wrangling\n\n\n\npopdata2023 &lt;- popdata |&gt;\n  dplyr::group_by(PA, SZ, AG) |&gt;\n  dplyr::summarise(`POP`=sum(`Pop`)) |&gt;\n  dplyr::ungroup() |&gt;\n  tidyr::pivot_wider(names_from = AG,\n              values_from = POP\n              )\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\"\n\n\n\n\n\n\nData ProcessingMutate the codeLEFT_JOIN\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\nmpsz_pop2023 &lt;- left_join(popdata2023 ,mpsz19_shp,\n                          by = c(\"SZ\"=\"SUBZONE_N\"))\n\nwrite_rds(mpsz_pop2023, \"data/rds/mpszpop2023.rds\")"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex02/In-Class_Ex02.html#working-with-master-plan-planning-sub-zone-data",
    "href": "In-Class_Ex/In-Class_Ex02/In-Class_Ex02.html#working-with-master-plan-planning-sub-zone-data",
    "title": "In-Class Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "The TaskTo import shapefileTo import kml\n\n\n\nVisit data.gov.sg and download Master Plan 2019 Subzone Boundary (No Sea) from the portal. You are required to download both kml files.\nMove MPSZ-2019 shapefile provided for In-Class Exercise 1 folder on elearn to data sub-folder of In-Class_Ex02.\nWrite a code chunk to import Master Plan 2019 Subzone Boundary (No Sea) kml and MPSZ-2019 into sf simple feature data.frame.\n\n\n\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\npacman::p_load(sf, tidyverse)\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz14_shp.\n\nmpsz19_shp &lt;- st_read(dsn = \"data\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\myhan\\IS415-GAA\\In-Class_Ex\\In-Class_Ex02\\data' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\npacman::p_load(sf, tidyverse)\n\n\nThe code chunk imports kml file. ::: {style=“font-size: 1.50em”}\n\n\nDeleting source `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting layer `MasterPlan2019SubzoneBoundaryNoSeaKML' to data source \n  `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting 332 features with 6 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\n\nPre-School Shape\n\n\n\npreschool &lt;- st_write(mpsz19_shp, \n                       \"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\",\n                       delete_dsn = TRUE) #incomplete\n\nDeleting source `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting layer `MasterPlan2019SubzoneBoundaryNoSeaKML' to data source \n  `data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' using driver `KML'\nWriting 332 features with 6 fields and geometry type Multi Polygon.\n\n\n\n\n\n\n\n\n\nThe Code\n\n\n\npopdata &lt;- readr::read_csv(\"data/aspatial/respopagesextod2023.csv\")\n\nRows: 100928 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nData Wrangling\n\n\n\npopdata2023 &lt;- popdata |&gt;\n  dplyr::group_by(PA, SZ, AG) |&gt;\n  dplyr::summarise(`POP`=sum(`Pop`)) |&gt;\n  dplyr::ungroup() |&gt;\n  tidyr::pivot_wider(names_from = AG,\n              values_from = POP\n              )\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\"\n\n\n\n\n\n\nData ProcessingMutate the codeLEFT_JOIN\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\n\n\nmpsz_pop2023 &lt;- left_join(popdata2023 ,mpsz19_shp,\n                          by = c(\"SZ\"=\"SUBZONE_N\"))\n\nwrite_rds(mpsz_pop2023, \"data/rds/mpszpop2023.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute spatial weights using R. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute spatial weights using R. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-study-area-and-data",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "2 5.2 The Study Area and Data",
    "text": "2 5.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n2.1 5.2.1 Getting Started\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-the-data-into-r-environment",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "3 5.3 Getting the Data Into R Environment",
    "text": "3 5.3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n3.1 5.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex05\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 5.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.3 5.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-regional-development-indicator",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "4 5.4 Visualising Regional Development Indicator",
    "text": "4 5.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-contiguity-spatial-weights",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "5 5.5 Computing Contiguity Spatial Weights",
    "text": "5 5.5 Computing Contiguity Spatial Weights\nIn this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n5.1 5.5.1 Computing (QUEEN) contiguity based neighbours\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 55 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,55)]\n\n[1] \"Hanshou\"  \"Jinshi\"   \"Li\"       \"Nan\"      \"Yongshun\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n5.2 5.5.2 Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n5.3 5.5.3 Visualising contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe check the first few observations to see if things are formatted correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n5.5.3.1 Plotting Queen contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n\n\n\n5.5.3.2 Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n5.5.3.3 Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-distance-based-neighbours",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "6 5.6 Computing distance based neighbours",
    "text": "6 5.6 Computing distance based neighbours\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n6.1 5.6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n6.2 5.6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nfor each point, it has an average of 3.681818 links to other points connected to it\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n5.6.2.1 Plotting fixed distance weight matrix\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n6.3 8.6.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotice that each county has six neighbours, no less no more!\n\n5.6.3.1 Plotting distance based neighbours\nWe can plot the weight matrix using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#weights-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#weights-based-on-idw",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "7 5.7 Weights based on IDW",
    "text": "7 5.7 Weights based on IDW\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#row-standardised-weights-matrix",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "8 5.8 Row-standardised Weights Matrix",
    "text": "8 5.8 Row-standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#application-of-spatial-weight-matrix",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "9 5.9 Application of Spatial Weight Matrix",
    "text": "9 5.9 Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n9.1 5.9.1 Spatial lag with row-standardized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.2 5.9.2 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n9.3 5.9.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nNote: For more effective comparison, it is advicible to use the core tmap mapping functions.\n\n\n9.4 5.9.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#references",
    "title": "Hands-On Exercise 5: Spatial Weights and Applications",
    "section": "10 5.10 References",
    "text": "10 5.10 References\n\nCreating Neighbours using sf objects"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "In general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this chapter, we will learn how to plot functional and truthful choropleth maps by using an R package called **tmap**."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "In general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this chapter, we will learn how to plot functional and truthful choropleth maps by using an R package called **tmap**."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "2 2.2 Getting Started",
    "text": "2 2.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data-into-r",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "3 2.3 Importing Data into R",
    "text": "3 2.3 Importing Data into R\n\n3.1 2.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 2.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo view the contents of mpsz, use the code chunk below:\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n3.3 2.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.4 2.3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n2.3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`,\n       `ECONOMY ACTIVE`, `AGED`,\n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n2.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;% \n  mutate_at(.vars = vars(PA, SZ),\n            .funs = list(toupper)) %&gt;%   filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "4 2.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "4 2.4 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n4.1 2.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n4.2 2.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n2.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n2.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n2.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")+\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n4.3 2.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes. (6 if you include the “Missing” Class)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\nIn the code chunk below, kmeans data classification method is used with 20 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, hclust data classification method is used with 10 classes used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nLastly, in the code chunk below, jenks data classification method is used with only 2 classes used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\nMy answer: Even by using the same classifications, but if the number of classes differs, it can be very misleading to see which districts have more plans for housing and whether they are building more houses. Just by looking at the previous map and hclust data classification method with 10 classes, it shows how big a difference in range for west coast District 0.00 - 1.33 in the 2 classes map is compared to the west coast District which is between 1.17 to 10.17 in the 10 classes map.\n\n\n2.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n\n\n\n\n4.4 2.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n2.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n4.5 2.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n2.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n2.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n4.6 2.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"ECONOMY ACTIVE\" ,\"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n2.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n\n\n\n\n2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n4.7 2.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 1.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "5 2.5 Reference",
    "text": "5 2.5 Reference\n\n5.1 2.5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n5.2 2.5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n5.3 2.5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a final-year university student currently studying at Singapore Management University, aspiring to be a geospatial data analyst with a passion for exploring the complex relationships between location-based data and real-world applications.\n\n\n\nWith a solid foundation in R programming language, my work involves using tools like R, particularly in conjunction with tidyverse, sf, tmap, and other spatial packages, to perform detailed spatial analyses and create visually compelling maps.\n\n\n\nMy research interests span across multiple domains of geospatial analysis, including spatial data visualization, spatial econometrics, and the application of GIS in urban planning and environmental monitoring. I am particularly interested in using geospatial techniques to address challenges related to urbanisation, environmental sustainability, and public health.\n\n\n\n\nGeospatial Tools: Learning R for geospatial analysis, particularly using packages like tidyverse, sf, tmap, and raster.\nProgramming Languages: Strong command of R, with working knowledge of Python and SQL for geospatial data processing.\n\n\n\n\n\n\n\nAs I continue to grow in the field of geospatial analysis, I aim to further refine my skills in spatial econometrics and expand my knowledge of machine learning applications in geospatial contexts. My long-term goal is to contribute to the development of more efficient and sustainable spatial solutions that can have a meaningful impact on society.\n\n\n\nFeel free to connect with me through LinkedIn or GitHub to discuss geospatial analysis, collaborative projects, or any other related topics."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I am a final-year university student currently studying at Singapore Management University, aspiring to be a geospatial data analyst with a passion for exploring the complex relationships between location-based data and real-world applications.\n\n\n\nWith a solid foundation in R programming language, my work involves using tools like R, particularly in conjunction with tidyverse, sf, tmap, and other spatial packages, to perform detailed spatial analyses and create visually compelling maps.\n\n\n\nMy research interests span across multiple domains of geospatial analysis, including spatial data visualization, spatial econometrics, and the application of GIS in urban planning and environmental monitoring. I am particularly interested in using geospatial techniques to address challenges related to urbanisation, environmental sustainability, and public health.\n\n\n\n\nGeospatial Tools: Learning R for geospatial analysis, particularly using packages like tidyverse, sf, tmap, and raster.\nProgramming Languages: Strong command of R, with working knowledge of Python and SQL for geospatial data processing.\n\n\n\n\n\n\n\nAs I continue to grow in the field of geospatial analysis, I aim to further refine my skills in spatial econometrics and expand my knowledge of machine learning applications in geospatial contexts. My long-term goal is to contribute to the development of more efficient and sustainable spatial solutions that can have a meaningful impact on society.\n\n\n\nFeel free to connect with me through LinkedIn or GitHub to discuss geospatial analysis, collaborative projects, or any other related topics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package.\nNote: Students are encouraged to read the reference guide of each function, especially the input data requirements, syntaxt and argument option before using them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#learning-outcome",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this hands-on exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package.\nNote: Students are encouraged to read the reference guide of each function, especially the input data requirements, syntaxt and argument option before using them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-acquisition",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-acquisition",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.2 Data Acquisition",
    "text": "1.2 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this hands-on exercise, you are required to extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\nNote: The purpose of this section is not merely extracting the necessary data sets. It also aims to introduce you to public available data sets. Students are encouraged to explore the rest of the available data sets in these three data sources.\n\n\n1.2.1 Extracting the geospatial data sets\nNext, at the Hands-on_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n1.2.2 Extracting the aspatial data set\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\nIn this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nType the following code chunk.\n\npacman::p_load(sf, tidyverse)\n\nWhat we can learn from the code chunk above:\n\np_load function pf pacman package is used to install and load sf and tidyverse pacages into R environment."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.4 Importing geospatial Data",
    "text": "1.4 Importing geospatial Data\nIn this section, you will learn how to import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n1.4.1 Importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message above reveals that the geospatial objects are multipolygon features. There are a total of 323 multipolygon features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinates systems. The bounding box provides the x extend and y extend of the data.\n\n\n1.4.2 Importing polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2024\", layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\CyclingPath_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n\n1.4.3 Importing GIS data in kml format\nThe PreSchoolsLocation is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 2290 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.5 Checking the Content of A Simple Feature Data Frame",
    "text": "1.5 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n1.5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n1.5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n1.5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nNote: One of the useful argument of head() is it allows user to select the numbers of record to display (i.e. the n argument)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\nNote: plot() is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.7 Working with Projection",
    "text": "1.7 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n1.7.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n1.7.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nGeometry set for 2290 features  Geometry type: POINT Dimension:     XYZ Bounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134 z_range:       zmin: 0 zmax: 0 Geodetic CRS:  WGS 84 First 5 geometries:\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\nNext, let us display the content of preschool3414 sf data frame as shown below.\nGeometry set for 2290 features  Geometry type: POINT Dimension:     XYZ Bounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88 z_range:       zmin: 0 zmax: 0 Projected CRS: SVY21 / Singapore TM First 5 geometries:\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.8 Importing and Converting An Aspatial Data",
    "text": "1.8 Importing and Converting An Aspatial Data\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n1.8.1 Importing the aspatial data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nRows: 3540 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,540 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 2  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 4 275343 10min wa… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Comforta… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Relaxing… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 344803 Budget s…  367042 Belinda   East Region         Tampines          1.35\n# ℹ 3,530 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n1.8.2 Creating a simple feature data frame from an aspatial data frame\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.9 Geoprocessing with sf package",
    "text": "1.9 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n1.9.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\nMission Accomplished!\n\n\n1.9.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nWarning: You should not confuse with st_intersection().\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\n\nDIY: Calculate the density of pre-school by planning subzone.\n\nThe solution:\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "1.10 Exploratory Data Analysis (EDA)",
    "text": "1.10 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nDIY: Using ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nThe solution:\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#the-specific-questions-we-would-like-to-answer-are-as-follows",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#the-specific-questions-we-would-like-to-answer-are-as-follows",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "2 3.1 The specific questions we would like to answer are as follows:",
    "text": "2 3.1 The specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#the-data",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3 3.2 The data",
    "text": "3 3.2 The data\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-loading-the-r-packages",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "4 3.3 Installing and Loading the R packages",
    "text": "4 3.3 Installing and Loading the R packages\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages.\n\n#|eval: false # it will no longer run anymore after first run\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\npackage 'maptools' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\mngyn\\AppData\\Local\\Temp\\Rtmp482JYG\\downloaded_packages\n\n\n\npacman::p_load(tmap, sf, tidyverse, raster, spatstat, maptools, sp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5 3.4 Spatial Data Wrangling",
    "text": "5 3.4 Spatial Data Wrangling\n\n5.1 3.4.1 Importing the spatial data\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R.\n\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex03\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex03\\data' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex03\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data.\n\nNotice that except childcare_sf, both mpsz_sf and sg_sf do not have proper crs information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\n\nDIY: If necessary, changing the referencing system to Singapore national projected coordinate system.\n\n\n\n5.2 3.4.2 Mapping the geospatial data sets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map.\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n# qtm(childcare_sf, fill = \"geometry\")\n# qtm(sg_sf)\n# qtm(mpsz_sf)\n\ntm_shape(sg_sf) + \n  tm_borders() +\ntm_shape(mpsz_sf) + \n  tm_polygons() +\ntm_shape(childcare_sf) + \n  tm_dots()\n\n\n\n\n\n\n\n\nNotice that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\n# tmap_mode('view')\n# tm_shape(childcare_sf)+\n#   tm_dots()\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6 3.5 Geospatial Data wrangling",
    "text": "6 3.5 Geospatial Data wrangling\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n6.1 3.5.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nDIY: Using appropriate function, display the information of these three Spatial* classes as shown below.\n\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\nNotice that the geospatial data have been converted into their respective sp’s Spatial* classes now.\n\n\n6.2 3.5.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nNext, you should display the sp objects properties as shown below.\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \n\n\n\nChallenge: Do you know what are the differences between Spatial* classes and generic sp object?\nAnswer: It removes the number of variables, names, min and max values.\n\n\n\n6.3 3.5.3 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\nWarning in as.ppp.sf(childcare_sf): only first attribute column is used for\nmarks\n\nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are of storage type  'character'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the different.\n\n# plot(childcare_ppp)\n\nYou can take a quick look at the summary statistics of the newly created ppp object by using the code chunk below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significant is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident.\n\n\n6.4 3.5.4 Handling duplicated points\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] FALSE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\nmultiplicity(childcare_ppp)\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [815] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [852] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [889] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [926] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [963] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1000] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1037] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1074] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1111] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1148] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1185] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1222] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1259] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1296] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1333] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1370] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1407] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1444] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1481] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[1518] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 0\n\n\nThe output shows that there are 0 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\n# tmap_mode('view')\n# tm_shape(childcare) +\n#   tm_dots(alpha=0.4, \n#           size=0.05)\n\n\ntmap_mode('plot')\n\ntmap mode set to plotting\n\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nDIY: Using the method you learned in previous section, check if any dusplicated point in this geospatial data.\n\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n6.5 3.5.5 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe ouput object can be displayed by using plot() function\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\nand summary() function of Base R.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n6.6 3.5.6 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1545 character character \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\nDIY: Using the method you learned in previous exercise, plot the newly derived childcareSG_ppp.\n\n\nplot(childcareSG_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1545 symbols are shown in the symbol map"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#first-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#first-order-spatial-point-patterns-analysis",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7 3.6 First-order Spatial Point Patterns Analysis",
    "text": "7 3.6 First-order Spatial Point Patterns Analysis\nIn this section, you will learn how to perform first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n7.1 3.6.1 Kernel Density Estimation\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n3.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\n3.6.1.2 Rescalling KDE values\nIn the code chunk below, rescale.ppp() is used to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\nNotice that output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n7.2 3.6.2 Working with different automatic badwidth methods\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle and bw.ppl methods\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n7.3 3.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel\n\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\nWarning in density.ppp(childcareSG_ppp.km, sigma = bw.ppl, edge = TRUE, :\nBandwidth selection will be based on Gaussian kernel"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "8 3.7 Fixed and Adaptive KDE",
    "text": "8 3.7 Fixed and Adaptive KDE\n\n8.1 3.7.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n8.2 4.7.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n8.3 3.7.3 Converting KDE output into grid object.\nThe result is the same, we just convert it so that it is suitable for mapping purposes.\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\nNote: as.SpatialGridDataFrame.im requires package “maptools” to use.\n\n3.7.3.1 Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\nNotice that the crs property is NA.\n\n\n3.7.3.2 Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -8.476185e-15, 28.51831  (min, max)\n\n\nNotice that the crs property is completed.\n\n\n\n8.4 3.7.4 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v”” field.\n\n\n8.5 3.7.5 Comparing Spatial Point Patterns using KDE\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n3.7.5.1 Extracting study area\nThe code chunk below will be used to extract the target planning areas.\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nPlotting target planning areas\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Punggol\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\nWarning: plotting the first 10 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\n\n\n\n\n\n3.7.5.2 Creating owin object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n3.7.5.3 Combining childcare points and the study area\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\nplot(childcare_tm_ppp.km, main=\"Tampines\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 89 symbols are shown in the symbol map\n\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 61 symbols are shown in the symbol map\n\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 88 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\n\n\n3.7.5.4 Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n3.7.5.5 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\npar(mar = c(3,2,2,1))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9 3.8 Nearest Neighbour Analysis",
    "text": "9 3.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n9.1 3.8.1 Testing spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nWhat conclusion can you draw from the test result?\nThe R value from the above result indicates clustering, and as p &lt; 0.05, H0 can be rejected.\nPerforming the same test on the target planning areas:\nTampines\n\nclarkevans.test(childcare_tm_ppp, correction = \"none\", clipregion = \"sg_owin\", alternative = c(\"clustered\"), nsim = 99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.78659, p-value = 5.868e-05\nalternative hypothesis: clustered (R &lt; 1)\n\n\nThe R value from the above result indicates clustering, and as p &lt; 0.05, H0 can be rejected.\nChoa Chu Kang\n\nclarkevans.test(childcare_ck_ppp, correction = \"none\", clipregion = \"sg_owin\", alternative = c(\"clustered\"), nsim = 99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.91976, p-value = 0.1153\nalternative hypothesis: clustered (R &lt; 1)\n\n\nAs p &gt; 0.05, H0 cannot be rejected.\nPunggol\n\nclarkevans.test(childcare_pg_ppp, correction = \"none\", clipregion = \"sg_owin\", alternative = c(\"clustered\"), nsim = 99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_pg_ppp\nR = 0.91095, p-value = 0.09167\nalternative hypothesis: clustered (R &lt; 1)\n\n\nAs p &lt; 0.05, H0 can be rejected.\nJurong West\n\nclarkevans.test(childcare_jw_ppp, correction = \"none\", clipregion = \"sg_owin\", alternative = c(\"clustered\"), nsim = 99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_jw_ppp\nR = 0.88419, p-value = 0.01884\nalternative hypothesis: clustered (R &lt; 1)\n\n\nSimilarily to Punggol, as p &lt; 0.05, H0 can be rejected."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nd-order-spatial-point-pattern-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nd-order-spatial-point-pattern-analysis",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "10 4. 2nd Order Spatial Point Pattern Analysis",
    "text": "10 4. 2nd Order Spatial Point Pattern Analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "11 4. Analysing Spatial Point Process Using G-Function",
    "text": "11 4. Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n11.1 4.1.1 Choa Chu Kang planning area\n\n4.1.1.1 Computing G-function estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n4.1.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nMonte Carlo test with G-fucntion\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n\n11.2 4.1.2 Tampines planning area\n\n4.1.2.1 Computing G-function estimation\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\n\n\n4.1.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "12 4.2 Analysing Spatial Point Process Using F-Function",
    "text": "12 4.2 Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n12.1 4.2.1 Choa Chu Kang planning area\n\n4.2.1.1 Computing F-function estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp) \nplot(F_CK)\n\n\n\n\n\n\n\n\n\n\n\n12.2 4.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nMonte Carlo test with F-fucntion\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n12.3 4.2.3 Tampines planning area\n\n4.2.3.1 Computing F-function estimation\nMonte Carlo test with F-fucntion\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\") \nplot(F_tm)\n\n\n\n\n\n\n\n\n\n\n4.2.3.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "13 4.3 Analysing Spatial Point Process Using K-Function",
    "text": "13 4.3 Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n13.1 4.3.1 Choa Chu Kang planning area\n\n4.3.1.1 Computing K-fucntion estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\") \nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n4.3.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n13.2 4.3.2 Tampines planning area\n\n4.3.2.1 Computing K-fucntion estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\") \nplot(K_tm, . -r ~ r,       \n     ylab= \"K(d)-r\", xlab = \"d(m)\",       \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\nPerform a complete Spatial Randomness Test with the following hypotheses:\n\nH0: Random distribution of childcare centres in Tampines\nH1: Non-Random distribution of childcare centres in Tampines\n\nReject H0 if p &lt; 0.001.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r,       \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-On Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "14 4.4 Analysing Spatial Point Process Using L-Function",
    "text": "14 4.4 Analysing Spatial Point Process Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n14.1 4.4.1 Choa Chu Kang planning area\n\n4.4.1.1 Computing L Fucntion estimation\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\") \nplot(L_ck, . -r ~ r,       \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\n\n\n4.4.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n\n14.2 4.4.2 Tampines planning area\n\n4.4.2.1 Computing L-fucntion estimate\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\") \nplot(L_tm, . -r ~ r,       \n     ylab= \"L(d)-r\", xlab = \"d(m)\",       \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\n\n\n4.4.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "0.2 Getting Started",
    "text": "0.2 Getting Started\n\n0.2.1 The analytical question\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n0.2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n0.2.3 Setting the Analytical Toolls\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-the-data-into-r-environment",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "0.3 Getting the Data Into R Environment",
    "text": "0.3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n0.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n0.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n0.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  dplyr::select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n0.3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal Interval Classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal Quantile Classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "0.4 Global Measures of Spatial Autocorrelation",
    "text": "0.4 Global Measures of Spatial Autocorrelation\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n0.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n0.4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nremark\n\n\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "0.5 Global Measures of Spatial Autocorrelation: Moran’s I",
    "text": "0.5 Global Measures of Spatial Autocorrelation: Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n0.5.1 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nBased on the Moran’s I statistic and the very low p-value, I conclude that there is a statistically significant positive spatial autocorrelation in the GDPPC data. This means that the GDPPC values are not randomly distributed across space. Instead, there is a pattern where similar values are clustered together geographically.\n\n\n0.5.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nSince the p-value is very low (0.001), reject the null hypothesis of no spatial autocorrelation. The result indicates strong evidence of a statistically significant positive spatial autocorrelation in the GDPPC data. This means the GDPPC values are not randomly distributed across the regions; rather, similar values are spatially clustered.\n\n\n0.5.3 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\nQuestion: What statistical observation can you draw fro mthe output above?\n\n\nChallenge: Instead of using Base Graph to plot the values, plot the values by using ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "0.6 Global Measures of Spatial Autocorrelation: Geary’s C",
    "text": "0.6 Global Measures of Spatial Autocorrelation: Geary’s C\nIn this section, you will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n0.6.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nGiven the Geary’s C value of 0.6907 (less than 1) and a very low p-value, I conclude that there is statistically significant positive spatial autocorrelation in the GDPPC data. This means that regions with similar GDPPC values are more likely to be located near each other, confirming a clustering pattern in the data.\n\n\n0.6.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n Since the p-value is very low (0.001), reject the null hypothesis of no spatial autocorrelation. This result suggests that the observed Geary’s C is highly unlikely to occur by random chance.\n\n\n0.6.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\n\n\nQuestion: What statistical observation can you draw from the output?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-correlogram",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "0.7 Spatial Correlogram",
    "text": "0.7 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n0.7.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe spatial correlogram shows that GDPPC values are strongly positively autocorrelated at short distances (lags 1 and 2), indicating that nearby regions have similar GDPPC values. As the distance increases, this positive autocorrelation weakens and turns into negative spatial autocorrelation at higher lags (5 and 6), suggesting that regions further apart tend to have dissimilar GDPPC values. This transition from positive to negative spatial autocorrelation reflects a spatial pattern in the distribution of GDPPC across the regions.\n\n\n0.7.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#local-indicators-of-spatial-associationlisa",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#local-indicators-of-spatial-associationlisa",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "1.1 Local Indicators of Spatial Association(LISA)",
    "text": "1.1 Local Indicators of Spatial Association(LISA)\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters and/or outliers in the spatial arrangement of a given variable. For instance if we are studying distribution of GDP per capita of Hunan Provice, People Republic of China, local clusters in GDP per capita mean that there are counties that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n1.1.1 Computing Contiguity Spatial Weights\nBefore we can compute the local spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n1.1.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n1.1.3 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\nMapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\nMapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nMapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-a-lisa-cluster-map",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "1.2 Creating a LISA Cluster Map",
    "text": "1.2 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n1.2.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n1.2.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\n1.2.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])\n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n1.2.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\nQuestion: What statistical observations can you draw from the LISA map above?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "1.3 Hot Spot and Cold Spot Area Analysis",
    "text": "1.3 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n1.3.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n1.3.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\nDeriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\n\nWarning in knn2nb(knearneigh(coords)): neighbour object has 25 sub-graphs\n\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n1.3.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nconvert nb object into spatial weight object\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-gi-statistics",
    "title": "Hands-On_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "1.4 Computing Gi statistics",
    "text": "1.4 Computing Gi statistics\n\n1.4.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n1.4.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\n\n\n1.4.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n1.4.4 Mapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\nI observed that therer is significant spatial clusters of high and low GDP per capita, indicating economic hotspots (in red) and coldspots (in blue). These clusters show statistically significant patterns of wealth disparity, with higher GDP concentrated in central and eastern regions, while lower GDP is more likely seen in southern areas. The map also reveals clear spatial inequality, suggesting targeted economic development may be necessary to address these regional imbalances."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this In-Class exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this In-Class exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#learning-outcome",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#learning-outcome",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "Geospatial Data Science is a process of importing, wrangling, integrating, and processing geographically referenced data sets. In this In-Class exercise, you will learn how to perform geospatial data science tasks in R by using sf package.\nBy the end of this In-Class exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#data-acquisition",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#data-acquisition",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "1.2 Data Acquisition",
    "text": "1.2 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this In-Class exercise, you are required to extract the necessary data sets from the following sources:\n\nMPSZ on elearn\n\n\n1.2.1 Extracting the geospatial data sets\nNext, at the In-Class_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n1.2.2 Extracting the aspatial data set\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#getting-started",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\nTwo R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. The following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nType the following code chunk.\n\npacman::p_load(sf, tidyverse)\n\n\nThe p_load function pf pacman package is used to install and load sf and tidyverse pacages into R environment."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#importing-geospatial-data",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#importing-geospatial-data",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "1.4 Importing Geospatial Data",
    "text": "1.4 Importing Geospatial Data\nWe will import the following geospatial data into R by using st_read() of sf package:\n-   `MP14_SUBZONE_WEB_PL`, a polygon feature layer in ESRI shapefile format,\n\n-   `CyclingPath`, a line feature layer in ESRI shapefile format, and\n\n-   `PreSchool`, a point feature layer in kml file format.\n\n1.4.1 Importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\nmpsz = st_read(dsn = \"C:/IS415-GAA/In-Class_Ex/In-Class_Ex01/data\", \n                  layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `C:\\IS415-GAA\\In-Class_Ex\\In-Class_Ex01\\data' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nThere are a total of 332 multipolygon features and 6 fields in mpsz simple feature data frame. mpsz is in WGS84 projected coordinates systems."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "1.5 Checking the Content of A Simple Feature Data Frame",
    "text": "1.5 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will use different ways to retrieve information related to the content of a simple feature data frame.\n\n1.5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 332 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((103.8802 1.283859, 103.8802 1.2...\n\n\nMULTIPOLYGON (((103.8376 1.295599, 103.8377 1.2...\n\n\nMULTIPOLYGON (((103.8341 1.292476, 103.8341 1.2...\n\n\nMULTIPOLYGON (((103.7125 1.291625, 103.7126 1.2...\n\n\nMULTIPOLYGON (((103.8472 1.297, 103.8473 1.2969...\n\n\n\n\n1.5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 332\nColumns: 7\n$ SUBZONE_N  &lt;chr&gt; \"MARINA EAST\", \"INSTITUTION HILL\", \"ROBERTSON QUAY\", \"JURON…\n$ SUBZONE_C  &lt;chr&gt; \"MESZ01\", \"RVSZ05\", \"SRSZ01\", \"WISZ01\", \"MUSZ02\", \"MPSZ05\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA EAST\", \"RIVER VALLEY\", \"SINGAPORE RIVER\", \"WESTERN …\n$ PLN_AREA_C &lt;chr&gt; \"ME\", \"RV\", \"SR\", \"WI\", \"MU\", \"MP\", \"WI\", \"WI\", \"SI\", \"SI\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"WEST…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"WR\", \"CR\", \"CR\", \"WR\", \"WR\", \"CR\", \"CR\",…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((103.8802 1...., MULTIPOLYGON (…\n\n\n\n\n1.5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R. In this case we will look at the first 5 rows of mpsz\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6537 ymin: 1.216215 xmax: 103.8811 ymax: 1.29742\nGeodetic CRS:  WGS 84\n                SUBZONE_N SUBZONE_C      PLN_AREA_N PLN_AREA_C       REGION_N\n1             MARINA EAST    MESZ01     MARINA EAST         ME CENTRAL REGION\n2        INSTITUTION HILL    RVSZ05    RIVER VALLEY         RV CENTRAL REGION\n3          ROBERTSON QUAY    SRSZ01 SINGAPORE RIVER         SR CENTRAL REGION\n4 JURONG ISLAND AND BUKOM    WISZ01 WESTERN ISLANDS         WI    WEST REGION\n5            FORT CANNING    MUSZ02          MUSEUM         MU CENTRAL REGION\n  REGION_C                       geometry\n1       CR MULTIPOLYGON (((103.8802 1....\n2       CR MULTIPOLYGON (((103.8376 1....\n3       CR MULTIPOLYGON (((103.8341 1....\n4       WR MULTIPOLYGON (((103.7125 1....\n5       CR MULTIPOLYGON (((103.8472 1...."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#plotting-the-geospatial-data",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex01.html#plotting-the-geospatial-data",
    "title": "In-Class Exercise 1: Geospatial Data Science with R",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features.\n\nplot(mpsz)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex03/In-Class_Ex03.html",
    "href": "In-Class_Ex/In-Class_Ex03/In-Class_Ex03.html",
    "title": "In-Class Exercise 3: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "",
    "text": "This exercise focuses on applying geospatial analytics to explore and analyze the impact of the armed conflict in Myanmar. The aim is to use spatial data and analytical techniques to better understand the conflict’s dynamics, identify affected regions, and assess the humanitarian implications. This case study offers a real-world application of geospatial tools for social good, demonstrating how data-driven insights can inform decision-making in crisis situations.\n\n\nTo provide answers to the questions above, the data set that will be used is:\n\nACLED_Myanmar.csv in the data folder\n\nTools used:\n\nacledata requires Access Key, Given out later\n\n\n\n\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat. (Deprecated as of October 2023)\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex03/In-Class_Ex03.html#data-used",
    "href": "In-Class_Ex/In-Class_Ex03/In-Class_Ex03.html#data-used",
    "title": "In-Class Exercise 3: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "",
    "text": "To provide answers to the questions above, the data set that will be used is:\n\nACLED_Myanmar.csv in the data folder\n\nTools used:\n\nacledata requires Access Key, Given out later"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex03/In-Class_Ex03.html#installing-and-loading-the-r-packages",
    "href": "In-Class_Ex/In-Class_Ex03/In-Class_Ex03.html#installing-and-loading-the-r-packages",
    "title": "In-Class Exercise 3: Geospatial analytics for Social Good - Myanmar Arm Conflict Case Study",
    "section": "",
    "text": "In this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat. (Deprecated as of October 2023)\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the five R packages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA ~ Han Ming Yan",
    "section": "",
    "text": "Welcome to my IS415 Geospatial Analytics and Applications website! Here, you will find my coursework prepared for this course.\nBrief Introduction:\nMy name is Ming Yan and I am currently a Final Year Computer Science Undergraduate studying at Singapore Management University (SMU).\nProject: ASEAN Geospatial Challenge 2025 (sla.gov.sg)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "href": "In-Class_Ex/In-Class_Ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     \n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\nThe analysis of spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Several spatio-temporal point patterns analysis methods have been introduced and implemented in R in the last ten years. This chapter shows how various R packages can be combined to run a set of spatio-temporal point pattern analyses in a guided and intuitive way. A real world forest fire events in Kepulauan Bangka Belitung, Indonesia from 1st January 2023 to 31st December 2023 is used to illustrate the methods, procedures and interpretations."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\nThe analysis of spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Several spatio-temporal point patterns analysis methods have been introduced and implemented in R in the last ten years. This chapter shows how various R packages can be combined to run a set of spatio-temporal point pattern analyses in a guided and intuitive way. A real world forest fire events in Kepulauan Bangka Belitung, Indonesia from 1st January 2023 to 31st December 2023 is used to illustrate the methods, procedures and interpretations."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#learning-outcome",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "2 Learning Outcome",
    "text": "2 Learning Outcome\n\n2.1 The research questions\nThe specific questions we would like to answer are:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-data",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "3 The data",
    "text": "3 The data\nFor the purpose of this exercise, two data sets will be used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e. kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-the-r-packages",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "4 Installing and Loading the R packages",
    "text": "4 Installing and Loading the R packages\nFor the purpose of this study, six R packages will be used. They are:\n\nsf provides functions for importing processing and wrangling geospatial data,,\nraster for handling raster data in R,\nspatstat for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc.,\nsparr provides functions to estimate fixed and adaptive kernel-smoothed spatial relative risk surfaces via the density-ratio method and perform subsequent inference. Fixed-bandwidth spatiotemporal density and relative risk estimation is also supported\ntmap provides functions to produce cartographic quality thematic maps, and\ntidyverse, a family of R packages that provide functions to perform common data science tasks including and not limited to data import, data transformation, data wrangling and data visualisation."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#load-the-r-packages-using-pacmanp_load",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#load-the-r-packages-using-pacmanp_load",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "5 Load the R packages using Pacman::p_load",
    "text": "5 Load the R packages using Pacman::p_load\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-and-preparing-study-area",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-and-preparing-study-area",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "6 Importing and Preparing Study Area",
    "text": "6 Importing and Preparing Study Area\n\n6.1 Importing study area\nCode chunk below is used import study area (i.e. Kepulauan Bangka Belitung) into R environment.\n\nkbb &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") \n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex04\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nIn sf format:\n\nkbb_sf &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union() %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `C:\\myhan\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex04\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nst_as_s2(): dropping Z and/or M coordinate\n\n\n\n\n6.2 Converting OWIN\nNext, as.owin() is used to convert kbb into an owin object.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nkbb_owin\n\nwindow: polygonal boundary\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n\n\nUse class() is used to confirm if the output is indeed an owin object.\n\nclass(kbb_owin)\n\n[1] \"owin\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-and-preparing-forest-fire-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-and-preparing-forest-fire-data",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "7 Importing and Preparing Forest Fire data",
    "text": "7 Importing and Preparing Forest Fire data\nNext, we will import the forest fire data set (i.e. forestfires.csv) into R environment.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                       crs = 4326) %&gt;%\n  st_transform(crs = 32748)\n\nRows: 741 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (3): satellite, instrument, daynight\ndbl  (11): latitude, longitude, brightness, scan, track, acq_time, confidenc...\ndate  (1): acq_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBecause ppp object only accept numerical or character as mark. The code chunk below is used to convert data type of acq_date to numeric.\n\nfire_sf &lt;- fire_sf %&gt;% \n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date, \n                           label = TRUE, \n                           abbr = FALSE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-the-fire-points",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-the-fire-points",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "8 Visualising the Fire Points",
    "text": "8 Visualising the Fire Points\n\n8.1 Overall plot\nWe will now prepare a point symbol map showing the distribution of fire points using this code chunk below.\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n8.2 Visuaising geographic distribution of forest fires by month\nNext, we will prepare a point symbol map showing the monthly geography distribution of fire points in 2023 using this code chunk below.\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots(size = 0.1) +\ntm_facets(by=\"Month_fac\", \n            free.coords=FALSE, \n            drop.units = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-month",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-month",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "9 Computing STKDE by Month",
    "text": "9 Computing STKDE by Month\nIn this section, you will learn how to compute STKDE by using spattemp.density() of sparr package. Before using the function, it is highly recommended you read the function’s reference guide in detail in order to understand the input data requirements and the output object generated.\n\n9.1 Extracting forest fires by month\nThe code chunk below is used to remove the unwanted fields from fire_sf sf data.frame. This is because as.ppp() only need the mark field and geometry field from the input sf data.frame.\n\nfire_month &lt;- fire_sf %&gt;% \n  select(Month_num)\n\n\n\n9.2 Creating ppp\nThe code chunk below is used to derive a ppp object called fire_month from fire_month sf data.frame.\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nCheck if the output is in the correct object class\n\nsummary(fire_month_ppp)\n\nMarked planar point pattern:  741 points\nAverage intensity 2.49258e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n                    (174200 x 170600 units)\nWindow area = 29728200000 square units\n\n\nCheck if there are duplicated point events by using the code chunk below.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\n9.3 Including Owin object\nThe code chunk below is used to combine origin_am_ppp and am_owin objects into one.\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\nAs a good practice, plot() is used to plot ff_owin so that we can examine the correctness of the output object.\n\nplot(fire_month_owin)\n\n\n\n\n\n\n\n\n\n\n9.4 Computing Spatio-temporal KDE\nNext, spattemp.density() of sparr package is used to compute the STKDE.\n\nst_kde &lt;- spattemp.density(fire_month_owin)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\n\n\n9.5 Plotting the spatio-temporal KDE object\nIn the code chunk below, plot() of R base is used to the KDE for between July 2023 - December 2023.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(2,3))\nfor(i in tims){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "10 Computing STKDE by Day of Year",
    "text": "10 Computing STKDE by Day of Year\nIn this section, you will learn how to computer the STKDE of forest fires by day of year.\n\n10.1 Creating ppp object\nIn the code chunk below, DayofYear field is included in the output ppp object.\n\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n10.2 Including Owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin]\nsummary(fire_yday_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 6.3198 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [3.959516e-27, 2.751287e-12]\n\n\nplot the kde_yday which is each day of the year\n\nplot(kde_yday)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year-improved-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year-improved-method",
    "title": "Hands-On_Ex04 - Spatio-Temporal Point Patterns Analysis",
    "section": "11 Computing STKDE by Day of Year: Improved method",
    "text": "11 Computing STKDE by Day of Year: Improved method\nOne of the nice function provides in sparr package is BOOT.spattemp(). It support bandwidth selection for standalone spatiotemporal density/intensity based on bootstrap estimation of the MISE, providing an isotropic scalar spatial bandwidth and a scalar temporal bandwidth.\nCode chunk below uses BOOT.spattemp() to determine both the spatial bandwidth and the scalar temporal bandwidth.\n\nset.seed(1234)\nBOOT.spattemp(fire_yday_owin) \n\nInitialising...Done.\nOptimising...\nh = 15102.47 \b; lambda = 16.84806 \nh = 16612.72 \b; lambda = 16.84806 \nh = 15102.47 \b; lambda = 1527.095 \nh = 15480.03 \b; lambda = 771.9715 \nh = 15668.81 \b; lambda = 394.4098 \nh = 15763.2 \b; lambda = 205.6289 \nh = 15810.4 \b; lambda = 111.2385 \nh = 15833.99 \b; lambda = 64.04328 \nh = 15845.79 \b; lambda = 40.44567 \nh = 15851.69 \b; lambda = 28.64687 \nh = 15863.49 \b; lambda = 5.049258 \nh = 15854.64 \b; lambda = 22.74746 \nh = 15860.54 \b; lambda = 10.94866 \nh = 15859.07 \b; lambda = 13.89836 \nh = 14348.82 \b; lambda = 13.89836 \nh = 13216.87 \b; lambda = 12.42351 \nh = 12460.27 \b; lambda = 15.37321 \nh = 10760.88 \b; lambda = 16.11064 \nh = 8875.282 \b; lambda = 11.68608 \nh = 10432.08 \b; lambda = 12.97658 \nh = 7976.084 \b; lambda = 16.66371 \nh = 9286.281 \b; lambda = 15.60366 \nh = 9615.08 \b; lambda = 18.73771 \nh = 9206.581 \b; lambda = 21.61828 \nh = 8140.483 \b; lambda = 18.23073 \nh = 8795.582 \b; lambda = 17.70071 \nh = 9124.381 \b; lambda = 20.83477 \nh = 9164.856 \b; lambda = 19.52699 \nh = 8345.358 \b; lambda = 18.48998 \nh = 9297.65 \b; lambda = 18.67578 \nh = 8928.375 \b; lambda = 16.8495 \nh = 9105.736 \b; lambda = 18.85762 \nDone.\n\n\n         h     lambda \n9105.73611   18.85762 \n\n\n\n11.1 Computing spatio-temporal KDE\nNow, the STKDE will be derived by using h and lambda values derive in previous step.\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin,\n  h = 9000,\n  lambda = 19)\n\nCalculating trivariate smooth...Done.\nEdge-correcting...Done.\nConditioning on time...Done.\n\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 9000 (spatial)\n  lambda = 19 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [2.001642e-19, 2.445724e-12]\n\n\n\n\n11.2 Plotting the output spatio-temporal KDE\nLast, plot() of sparr package is used to plot the output as shown below.\n\nplot(kde_yday)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "pacman::p_load(sf, tidyverse, tmap, sfdep)\n\n\n\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\myhan\\IS415-GAA\\In-Class_Ex\\In-Class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_GDPPC &lt;- left_join(hunan,hunan2012) %&gt;%\n  dplyr::select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_GDPPC, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  dplyr::mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, \n                         style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#getting-the-data-into-r-environment",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#getting-the-data-into-r-environment",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "The code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\myhan\\IS415-GAA\\In-Class_Ex\\In-Class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_GDPPC &lt;- left_join(hunan,hunan2012) %&gt;%\n  dplyr::select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#visualising-regional-development-indicator",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#visualising-regional-development-indicator",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Now, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_GDPPC, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  dplyr::mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, \n                         style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#performing-global-morans-i-test",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#performing-global-morans-i-test",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "2.1 Performing Global Moran’s I test",
    "text": "2.1 Performing Global Moran’s I test\nIn general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed using global_moran_test() as shown in the code chunk below.\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#performing-global-morans-i-permutation-test",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#performing-global-morans-i-permutation-test",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "2.2 Performing Global Moran’s I permutation test",
    "text": "2.2 Performing Global Moran’s I permutation test\nIn practice, Monte Carlo simulation should be used to perform the statistical test. For sfdep, it is supported by global_moran_perm()\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#computing-local-morans-i",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#computing-local-morans-i",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "2.3 Computing local Moran’s I",
    "text": "2.3 Computing local Moran’s I\nCompute Local Moran’s I of GDPPC at county level by using local_moran() of the sfdep package.\n\nlisa &lt;- wm_q %&gt;% \n  dplyr::mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;% \n  unnest(local_moran)\n\np_ii is base method, p_ii_sim is using the simulation method, p_folded_sim is using the k-fold method, p_ii_sim has no replacement, p_folded numbers are lower but roughly the method is still the same.\nTo ensure consistency, stay with 1 type of p-value (either p_ii_sim or p_folded_sim)\nkey thing is the mean and median, skewness can be both positive and negative numbers, i can sort it to make it easier to read\n\n# to measure skewness\nmedian(lisa$skewness) # -0.6968806\n\n[1] -0.6968806\n\n# to see the mean skewness across the whole data\nmean(lisa$skewness) # -0.2233379\n\n[1] -0.2233379\n\n# can also plot it out into a histogram to visualise better to see the skewness\n\n\nexcessive skewness -&gt; use median columns\nif close to 0 skewness -&gt; use mean columns"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#visualising-local-morans-i-and-p-value",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#visualising-local-morans-i-and-p-value",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "2.4 Visualising local Moran’s I and p-value",
    "text": "2.4 Visualising local Moran’s I and p-value\nthe next map is prepared using this chunk of code\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nmap1 &lt;- tm_shape(lisa) + \n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"Local Moran's I of GDPPC\",\n    main.title.size = 1.5)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"p-value of local Moran's I\",\n    main.title.size = 1.5\n  )\n\ntmap_arrange(map1, map2, ncol = 2)\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#computing-local-gi-statistics",
    "href": "In-Class_Ex/In-Class_Ex06/In-Class_Ex06.html#computing-local-gi-statistics",
    "title": "In-Class_Ex06 - Local & Global Measures of Spatial Autocorrelation",
    "section": "3.1 Computing local Gi* statistics",
    "text": "3.1 Computing local Gi* statistics\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(\n    nb = st_contiguity(geometry),\n    wts = st_inverse_distance(\n      nb, geometry,\n      scale = 1, alpha = 1\n    ),\n    .before = 1\n  )\n\n! Polygon provided. Using point on surface.\n\n\nWarning: There was 1 warning in `stopifnot()`.\nℹ In argument: `wts = st_inverse_distance(nb, geometry, scale = 1, alpha = 1)`.\nCaused by warning in `st_point_on_surface.sfc()`:\n! st_point_on_surface may not give correct results for longitude/latitude data\n\n\nwe will now compute the local Gi* by using the code chunk below\n\nHCSA &lt;- wm_idw %&gt;%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99\n  ), .before = 1) %&gt;%\n  unnest(local_Gi)\n\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;\n\n\n\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5)\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  }
]